% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Policy Gradient]{RL: Policy Search}
\subtitle{Policy Gradient Algorithms: Actor-Critic}



\begin{document}
	
	\maketitle
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Incorporating Baselines: Actor-Critic Methods}

\begin{itemize}
	\item We know the near-optimal baseline is:
 $$b(s_t) \approx \mathbb{E} [r_t + r_{t+1} +\ldots + r_{T-1}] = G_t$$
    \item That should look very familiar since we know: 
    $$ V(s)  = \mathbb{E}[G_t \mid s_t=s] $$
\end{itemize}
$\leadsto$ We can use value estimation to approximate the optimal baseline
\end{frame}
%-----------------------------------------------------------------------
% %----------------------------------------------------------------------
\begin{frame}[c]{Choosing the Baseline: Value Functions}
	
	\begin{itemize}
		\item Recall $Q$-function:
		$$Q^\pi(s,a) = \mathbb{E}_\pi [r_0 +  \gamma r_1 + \gamma^2 r_2 + \ldots \mid s_0 = s, a_0 = a ]$$
		\item State-value function can serve as a great baseline since it's often easier to estimate:
		\begin{eqnarray}
		V^\pi (s) &=& \mathbb{E}_\pi [r_0 +  \gamma r_1 + \gamma^2 r_2 + \ldots \mid s_0 = s]\nonumber\\	
		&=& \mathbb{E}_{a\sim\pi} [Q^\pi(s,a)]\nonumber
		\end{eqnarray}
		\item Advantage function: estimating the advantage of a given action over the state value:
		$$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s) $$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
% %----------------------------------------------------------------------
\begin{frame}[c]{”Vanilla” Policy Gradient Algorithm With Advantage Baseline}
	
	\begin{itemize}
		\item Initialize policy parameters $\theta$ and baseline $b$
		\item for iteration$=1,2,\ldots$ do
		\begin{itemize}
			\item Collect a set of $K$ trajectories by executing the current policy
			\item At each time step $t$ in each trajectory $\tau^k$, compute
			\begin{itemize}
				\item Return $G_t^k= \sum_{t'=t}^{T-1} r_{t'}^k$ and
				\item Advantage estimate (actual return - general return estimate) $\hat{A}^k_t = G^k_t - b(s^k_t) = G^k_t - G^t$ 
			\end{itemize}
			\item Re-fit the baseline by minimizing $b := \sum_k \sum_t || b(s^k_t) - G^k_t||^2$ (MSE)
			\item Update the policy, using a policy gradient estimate $\hat{g}$
			\begin{itemize}
				\item which is a sum over k: $\hat{A}_t \nabla_\theta \log \pi(a^k_t \mid s^k_t; \theta) $
				\item Apply gradient $\hat{g}$ by any DL-optimizer (e.g., SGD or ADAM)
			\end{itemize}
		\end{itemize}
		
	\end{itemize}
 
\end{frame}

\begin{frame}[c]{Actor-Critic Methods}
	
	\begin{itemize}
		\item Methods combining policy approximation (an "Actor") with value approximation (a "Critic") are called "Actor-Critic" approaches
        \item Many different configurations depending on the exact combination of policy and value learning methods -- previous slide shows a basic version
        \item Most modern policy gradient algorithms are Actor-Critic algorithms
        \item A big difference to REINFORCE: we now update over a set of trajectories at once instead of updating at each step
	\end{itemize}
	\bigskip
 $\leadsto$ More stable, highly parallelizable policy gradient algorithms
\end{frame}

% %-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}