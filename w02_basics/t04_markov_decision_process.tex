% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Reinforcement Learning: Basics]{RL: Basics}
\subtitle{The Markov Decision Process}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Markov Decision Process (MDP)}

\begin{itemize}
	\item Markov Decision Process is Markov Reward Process + actions
	\item Definition of MDP
	\begin{itemize}
		\item $S$ is a (finite) set of Markov states $s \in S$
		\item $A$ is a (finite) set of actions $a \in A$
		\item $P$ is dynamics/transition model for each action, that specifies $P(s_{t+1} = s' \mid s_t=s, a_t=a)$
		\item $R$ is a reward function 
		$R(s_t=s, a_t=a) = \mathbb{E}[r_t \mid s_t=s, a_t=a] $
		\begin{itemize}
			\item Sometimes R is also defined based on $(s)$ or on $(s,a,s')$
		\end{itemize}
		\item Discount factor $\gamma \in [0, 1]$
	\end{itemize}
	\item MDP is tuple $(S,A,P, R, \gamma)$
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MDP (cont'd)}
	
	\begin{itemize}
		\item MDP is tuple $(S,A,P, R, \gamma)$
		\item Optional components:
		\begin{itemize}
			\item $\rho_0: S \to \mathbb{R}^+$: a distribution of start states
			\begin{itemize}
				\item uniform distribution: the agent can start in any state -- implicit assumption of MDP definition above
				\item non-uniform distribution: the agent starts its episodes in only some of the states;\\ e.g., it's unlikely that a game will start in a terminal state
			\end{itemize}
		\pause
			\item $T \subset S$: set of terminal states
			\begin{itemize}
				\item important for episodic MDPs 
				\item or if there is not fixed horizon, but the episodes should be finite
			\end{itemize}
		\pause
			\item $\gamma$: discount factor
			\begin{itemize}
				\item important to quantify the importance of future 
				\smallskip
				\item some treat $\gamma$ as a hyperparameter and not part of the definition
				\item[$\leadsto$] different optimal policies can be found
				\item[$\leadsto$] depends on how the optimal policy is defined
			\end{itemize}
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Mars Rover as MDP}

\begin{center}
	\includegraphics[width=0.9\textwidth]{images/mars_rover.png}
\end{center}

\begin{itemize}
	\item $2$ deterministic actions: TryLeft and TryRight
\end{itemize}



\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MDP Policies}

\begin{itemize}
	\item Policy specifies what action to take in each state
	\begin{itemize}
		\item Can be deterministic or stochastic
	\end{itemize}
	\item For generality, consider as a conditional distribution
	\begin{itemize}
		\item Given a state, specifies a distribution over actions
	\end{itemize}
	\item Policy: $\pi(a \mid s) = P(a_t=a | s_t = s)$
\end{itemize}


\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MDP + Policy}

\begin{itemize}
	\item MDP + Policy $\pi(a \mid s)$ = Markov Reward Process
	\item Precisely, it is the MRP $(S,R^\pi, P^\pi, \gamma)$ where
	$$R^\pi (s) = \sum_{a\in A} \pi(a\mid s ) R(s,a) $$
   $$P^\pi (s'\mid s) = \sum_{a\in A} \pi(a \mid s) P(s' \mid s,a)$$
   \item Implies we can use same techniques to evaluate the value of a policy  for an MDP as we could to compute the value of a MRP, by defining a
   MRP with $R^\pi$ and $P^\pi$
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MDP Policy Evaluation, Iterative Algorithm}

\begin{itemize}
	\item Goal: For a given $\pi$, determine $V^\pi$
	\item iterative approach:
	\begin{itemize}
		\item Initialize $V_0(s) = 0 $ for all s
		\item For $k=1$ until convergence
		\begin{itemize}
			\item For all $s$ in $S$:
			$$V^\pi_k (s)  = r(s, \pi(s)) + \gamma \sum_{s'\in S} p(s'\mid s, \pi(s)) V_{k-1}^\pi (s')$$
		\end{itemize}
	\end{itemize}
	\item This is a Bellmann backup for a particular policy
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Practice: MDP 1 Iteration of Policy Evaluation, Mars Rover Example}

\begin{itemize}
	\item Dynamics: $p(s_6 |s_6 , a_1 ) = 0.5, p(s_7 |s_6 , a_1 ) = 0.5, \ldots$
	\item Reward: for all actions, $+1$ in state $s_1$ , $+10$ in state $s_7$ , $0$ otherwise
	\item Let $\pi(s) = a_1.\forall s$, assume $V^\pi_k =[1,0,0,0,0,0,10]$ and $k = 1$, $\gamma = 0.5$
			$$V^\pi_k  = r(s, \pi(s)) + \gamma \sum_{s'\in S} p(s'\mid s, \pi(s)) V_{k-1}^\pi (s')$$
\end{itemize}

\pause

\begin{eqnarray}
V^\pi_{k+1} (s_6) &=& 0  + \gamma [ p(s_6 \mid s_6, a_1) \cdot V^\pi_k(s_6) + p(s_7 \mid s_6 , a_1) \cdot V^\pi_k(s_7) ] \nonumber\\
\pause
&=& \gamma [0.5 \cdot 0 + 0.5 \cdot 10] \nonumber\\
\pause
&=& \gamma \cdot 5 \nonumber\\
\pause
&=& 2.5 \nonumber\\
\end{eqnarray}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MDP Control}

\begin{itemize}
	\item Compute the optimal policy
	$$ \pi^*(s)  \in \argmax_\pi V^\pi(s)$$
	\item There \alert{exists a unique optimal value function}
	\item In an infinite horizon problem (i.e. agents acts forever is), there exists an optimal policy for an MDP,\newline that is 
	\begin{itemize}
		\item deterministic
		\item stationary (does not depend on time step)
		\item Unique? $\leadsto$ Not necessarily, may have state-actions with identical optimal values
	\end{itemize}
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}


