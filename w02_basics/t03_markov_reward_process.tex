% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Reinforcement Learning: Basics]{RL: Basics}
\subtitle{The Markov Reward Process}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Markov Reward Process (MRP)}

\begin{itemize}
	\item Extend Markov Process by rewards
	\item Definition of Markov Reward Process (MRP) $M =(S, P, R, \gamma)$
	\begin{itemize}
		\item $S$ is a (finite) set of states ($s \in S$)
		\item P is dynamics/transition model that specifies $P(s_{t+1}=s' \mid s_t = s)$
		\item R is a reward function $R(s_t = s) = \mathbb{E}[r_t \mid s_t = s]$
		\item Discount factor $\gamma \in [0,1]$
	\end{itemize}
	\item Note: no actions
	\item If a finite number (N) of states, we can express $R$ as a vector
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Mars Rover as MRP}
	
\begin{center}
	\includegraphics[width=0.9\textwidth]{images/mars_rover_markov_process_2.png}
\end{center}

Rewards:
\begin{itemize}
	\item $+1$ in $s_1$, 
	\item $+10$ in $s_7$,
	\item $0$ in all other states
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Return \& Value Function}
	
	\begin{itemize}
		\item Definition of Horizon
		\begin{itemize}
			\item Number of time steps in each episode
			\item Can be infinite
			\item Otherwise called finite Markov reward process
		\end{itemize}
	\smallskip
	\pause
		\item Definition of Return: $G_t$ (for a MRP)
		\begin{itemize}
			\item Discounted sum of rewards from time step $t$ to horizon
			$$ G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \ldots$$
		\end{itemize}
	\smallskip
	\pause
	\item Definition of State Value Function: $V(s)$ for a MRP
	\begin{itemize}
		\item Expected return from starting in state s
		$$ V(s)  = \mathbb{E}[G_t \mid s_t=s] = \mathbb{E}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \ldots \mid s_t = s]$$
	\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Discount Factor}
	
$$ V(s)  = \mathbb{E}[G_t \mid s_t=s] = \mathbb{E}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \ldots \mid s_t = s]$$
	
	\begin{itemize}
		\item Mathematically convenient (avoid infinite returns and values)
		\item Humans often act as if thereâ€™s a discount factor $\gamma < 1$
		\item $\gamma = 0$: Only care about immediate reward
		\item $\gamma = 1$: Future reward is as beneficial as an immediate reward
		\item If episode lengths are always finite, can use $\gamma = 1$ (but don't have to)
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Mars Rover as MRP}
	
	\begin{center}
		\includegraphics[width=0.8\textwidth]{images/mars_rover_markov_process_2.png}
	\end{center}
	
	Rewards:
	\begin{itemize}
		\item $+1$ in $s_1$, 
		\item $+10$ in $s_7$,
		\item $0$ in all other states
	\end{itemize}
	
	Sample returns for $4$-step episodes, $\gamma=1/2$
	\begin{itemize}
		\item $s_4,s_5,s_6,s_7: 0 + \frac{1}{2}\cdot 0 + \frac{1}{4}\cdot 0 + \frac{1}{8}\cdot 10 = 1.25$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c]{Mars Rover as MRP}
	
	\begin{center}
		\includegraphics[width=0.9\textwidth]{images/mars_rover_markov_process_2.png}
	\end{center}
	
	Rewards:
	\begin{itemize}
		\item $+1$ in $s_1$, $+10$ in $s_7$, $0$ in all other states
	\end{itemize}

$$ V(s)  = \mathbb{E}[G_t \mid s_t=s] = \mathbb{E}[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \gamma^3 r_{t+3} + \ldots \mid s_t = s]$$

	\begin{itemize}
		\item[$\leadsto$] $V(s_1) = 1.53, V(s_2) = 0.37, \ldots, V(s_7) = 15.31$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Computing the Value of a Markov Reward Process}
	
	\begin{itemize}
		\item Could estimate by simulation:
		\begin{enumerate}
			\item Generate a large number of episodes
			\item Average returns
		\end{enumerate}
		\item Requires no assumption of Markov structure
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Computing the Value of a Markov Reward Process}
	
	\begin{itemize}
		\item Markov property yields additional structure
		\item MRP value function satisfies
	\end{itemize}

$$V(s) = R(s) + \gamma \sum_{s' \in S} P(s'\mid s) V(s')$$
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Matrix Form of Bellman Equation for MRP}

$$\begin{pmatrix}
V(s_1)\\
\ldots\\
V(s_n)
\end{pmatrix} =
\begin{pmatrix}
R(s_1)\\
\ldots\\
R(s_n)
\end{pmatrix} 
+ \gamma 
\begin{pmatrix}
P(s_1|s_1)  & \ldots &  P(s_n \mid s_1)\\
\ldots\\ 	& \ldots & \ldots \\
P(s_1| s_n) & \ldots & P(s_n \mid s_n)\\
\end{pmatrix} 
\begin{pmatrix}
V(s_1)\\
\ldots\\
V(s_n)
\end{pmatrix} 
$$

\pause

\begin{eqnarray}
V &=& R + \gamma P V\\
V  - \gamma P V &=& R\\
(1- \gamma P) V  &=& R\\
V &=& (1- \gamma P)^{-1} R
\end{eqnarray}

\pause
$\leadsto$ Solving directly requires taking a matrix inverse $O(n^3)$

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Iterative Algorithm for Computing Value of a MRP}

\begin{itemize}
	\item Dynamic Programming :
	\begin{itemize}
	\item Initialize $V_0(s) = 0$ for all $s$
	\item For $k=1$ until convergence
	\begin{itemize}
		\item For all $s$ in $S$
		$$V_k(s) = R(s) + \gamma \sum_{s' \in S} P(s' \mid s) V_{k-1}(s')$$
	\end{itemize}
	\end{itemize}

	\pause
	\smallskip
	\item Computational complexity: $O(|S|^2 )$ for each iteration $(|S| = n)$
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}


