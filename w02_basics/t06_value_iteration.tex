% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Reinforcement Learning: Basics]{RL: Basics}
\subtitle{Value Iteration}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MDP: Computing Optimal Policy and Optimal Value}

\begin{itemize}
	\item Policy iteration computes optimal value and policy
	\item Value iteration is another technique
	\begin{itemize}
		\item Idea: Maintain the optimal value of starting in a state $s$ if we have a finite 	number of steps k left in the episode
		\item Iterate to consider longer and longer episode
	\end{itemize}

\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Bellman Equation and Bellman Backup Operators}

\begin{itemize}
	\item Value function of a policy must satisfy the Bellman equation
	$$V^\pi (s) = R^\pi (s) + \gamma \sum_{s' \in S}  P^\pi(s' \mid s ) V^\pi (s') $$
	\item Bellman backup operator $B$
	\begin{itemize}
		\item Applied to a value function
		 \item 	Returns a new value function
		 \item 	Improves the value if possible
		 $$ BV(s) = \max_{a} [ R(s,a) + \gamma \sum_{s' \in S} p(s' \mid s,a)  V(s')  ]$$
		 \item $BV$ yields a value function over all states s
		 \item Note: Read $B$ as an operator applied to $V$
	\end{itemize}
	
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Value Iteration (VI)}

\begin{itemize}
	\item  Set $k = 1$
	\item Initialize $V_0(s) = 0$ for all states $s$
	\item Loop until convergence
	\begin{itemize}
		\item For each state $s$
		$$V_{k+1}(s) = \max_{a\in A } R(s,a) + \gamma \sum_{s' \in S } P(s' \mid s,a) V_k(s') $$
		\item View as Bellmann backup on the value function
		$$V_{k+1} = BV_k$$
		$$\pi_{k+1}(s) \in \argmax_{a \in A} R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) V_k(s') $$
	\end{itemize}
	
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Iteration as Bellman Operations}

\begin{itemize}
	\item  Bellman backup operator $B^\pi$ for a particular policy is defined as
	$$ B^\pi V (s) = R^\pi (s) + \gamma \sum_{s' \in S}  P^\pi(s' \mid s ) V (s')  $$
	\item Policy evaluation amounts to computing the fixed point of $B^\pi$
	\item To do policy evaluation, repeatedly apply the operator until $V$ stops changing
	$$V^\pi = B^\pi B^\pi B^\pi B^\pi \ldots B^\pi V$$
	\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Iteration as Bellman Operations}

\begin{itemize}
	\item  Bellman backup operator $B^\pi$ for a particular policy is defined as
	$$ B^\pi V (s) = R^\pi (s) + \gamma \sum_{s' \in S}  P^\pi(s' \mid s ) V (s')  $$
	\item To do policy improvement
	$$ \pi_{k+1} (s) \in \argmax_{a \in A} R(s,a) + \gamma \sum_{s'\in S} P(s' \mid s,a) V^{\pi_{k}}(s')$$

\end{itemize}

\end{frame}
%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Going back to Value Iteration (VI)}

\begin{itemize}
	\item  Set $k = 1$
	\item Initialize $V_0(s) = 0$ for all states $s$
	\item Loop until convergence
	\begin{itemize}
		\item For each state $s$
		$$V_{k+1}(s) = \max_{a\in A } R(s,a) + \gamma \sum_{s' \in S } P(s' \mid s,a) V_k(s') $$
		\item Equivalent in Bellman backup notation
				$$ V_{k+1} = BV_k$$
		\item To extract optimal policy if we can act for $k+1$ more steps
		$$\pi_{k+1}(s) \in \argmax_{a \in A} R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) V_{k+1}(s') $$
	\end{itemize}
	
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%%----------------------------------------------------------------------
%\begin{frame}[c]{Going back to Value Iteration (VI)}
%
%\begin{itemize}
%	\item  Yes, if discount factor $\gamma < 1$, or end up in a terminal state with
%	probability $1$
%	\item 	Bellman backup is a contraction if discount factor, $\gamma <  1$
%	\item If apply it to two different value functions, distance between value functions shrinks after applying Bellman equation to each
%	\item (Skip proof)
%\end{itemize}
%
%\end{frame}
%%-----------------------------------------------------------------------
%----------------------------------------------------------------------

%TODO: adding one slide with a direct comparison of equations and pros and cons of PI vs VI

\begin{frame}[c]{What you should know}

\begin{itemize}
	\item  Define MP, MRP, MDP, Bellman operator, Q-value, policy
	\item Be able to implement
	\begin{itemize}
		\item Value Iteration
		\item Policy Iteration
	\end{itemize}
	\item Which policy evaluation methods require the Markov assumption?
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}


