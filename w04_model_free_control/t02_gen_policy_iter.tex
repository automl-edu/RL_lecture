% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Model Free Control]{Model Free Control}
\subtitle{Generalized Policy Iteration}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recall Policy Iteration}

\begin{itemize}
	\item Initialize policy $\pi$
	\item Repeat:
	\begin{itemize}
		\item Policy evaluation: compute $V^\pi$
		\item Policy improvement: update $\pi$
	$$\pi'(s) \in \argmax_{a \in A} R(s,a) + \gamma \sum_{s' \in S} P(s' \mid s,a) V^\pi (s') = \argmax_{a \in A} Q^\pi (s,a) $$
	\end{itemize}
	\item Now want to do the above two steps \alert{without} access to the true dynamics and reward models
	\item Before we introduced methods for model-free policy evaluation
\end{itemize}

\end{frame}
%--------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Model-Free Policy Iteration}
	
	\begin{itemize}
		\item Initialize policy $\pi$
		\item Repeat:
		\begin{itemize}
			\item Policy evaluation: compute $Q^\pi$
			\item Policy improvement: update $\pi$
		\end{itemize}
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MC for On-Policy Q-Evaluation}
	
	\begin{itemize}
		\item Initialize $\forall s\in S, a\in A$:
		\begin{itemize}
			\item $N(s,a) = 0$
			\item $G(s,a) = 0$
			\item $Q^\pi(s,a) =0$
		\end{itemize}
		\item Loop
		\begin{itemize}
			\item Using policy $\pi$ sample episode $i = s_{i,1}, a_{i,1}, r_{i,1}, s_{i,2}, a_{i,2}, r_{i,2}, \ldots, s_{i, T_i}$
			\item $G_{i,t} = r_{i,t} + \gamma r_{i,t+1}, \gamma^2 r_{i,t+2} + \ldots \gamma^{T_i -1} r_{i,T_i}$
			\item For each pair $(s,a)$ visited in episode $i$
			\begin{itemize}
				\item For first (or every) time $t$ that $(s,a)$ is visited in episode $i$:
				\item $N(s,a) = N(s,a) + 1$
				\item $G(s,a) = G(s,a) + G_{i,t}$
				\item Update estimate $Q^\pi(s,a) = G(s,a) /N(s,a)$
			\end{itemize}
		\end{itemize}
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Model-Free Generalized Policy Improvement}
	
	\begin{itemize}
		\item Given an estimate $Q^{\pi_i}(s,a) \forall s\in S, a \in A$
		\item Update new policy
		$$\pi_{i+1}(s) \in \argmax_{a \in A} Q^{\pi_i} (s,a) $$
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
