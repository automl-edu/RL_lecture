% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Model Free Control]{Model Free Control}
\subtitle{SARSA and Q-Learning}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Model-free Policy Iteration with TD Methods}
	
	\begin{itemize}
		\item Use temporal difference methods for policy evaluation step
		\item Initialize policy $\pi$
		\item Repeat:
		\begin{itemize}
			\item Policy evaluation: compute $Q^\pi$ using temporal difference updating
			with $Q$-greedy policy
			\item Policy improvement: Same as Monte Carlo policy improvement, set $\pi$
			to $\epsilon$-greedy ($Q^\pi$)
		\end{itemize}
		\item First consider SARSA, which is an on-policy algorithm
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{General Form of SARSA Algorithm}
	
	\begin{itemize}
		\item Initialization:
		\begin{itemize}
			\item $\epsilon$-greedy policy 
			\item $t=0$
			\item initial state $s_t = s_0$
		\end{itemize} 
		\item Loop
		\begin{itemize}
			\item Take action $a_t \sim \pi(s_t)$
			\item Observe $(r_t, s_{t+1})$, $a_{t+1} \sim \pi(s_{t+1})$
			\item Update Q given $(s_t, a_t, r_t, s_{t+1}, a_{t+1})$:
			$$Q(s_t,a_t) \gets Q(s_t, a_t) + \alpha (r_t + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$$
			\item $\pi(s_t) \in \argmax_{a\in A} Q(s_t, a)$ with probability $1-\epsilon$, else random
			\item $t = t+1$
		\end{itemize} 
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Convergence Properties of SARSA}
	
	\begin{itemize}
		\item Theorem:\\
		SARSA for finite-state and finite-action MDPs converges to the optimal
		action-value, $Q(s, a) \to  Q^*(s, a)$, under the following conditions:
		\begin{enumerate}
			\item The policy sequence $\pi_t(a \mid s)$ satisfies the condition of GLIE
			\item The step-sizes $\alpha_t$ satisfy the \alert{Robbins-Munro sequence} such that
		\end{enumerate}
	\end{itemize}
	\begin{eqnarray}
	\sum_{t=1}^{\infty} \alpha_t = \infty \nonumber \\
	\sum_{t=1}^{\infty} \alpha^2_t < \infty \nonumber
	\end{eqnarray}
	
	\begin{itemize}
		\item For example $\alpha_t= \frac{1}{t}$ satisfies the above condition
		\pause
		\item Would one want to use a step size choice that satisfies the above in
		practice? Likely not.
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Q-Learning: Learning the Optimal State-Action Value}
	
	\vspace{-1em}
	\begin{itemize}
		\item SARSA is an on-policy learning algorithm
		\item SARSA estimates the value of the current behavior policy\\ (policy
		used to take actions in the world)
		\item And then updates the policy trying to estimate
		\pause
		\medskip
		\item Alternatively, can we directly estimate the value of $\pi^*$\\ while acting with another behavior policy $\pi_b$?
		\item Yes! Q-learning, an off-policy RL algorithm
		\item Key idea: Maintain state-action Q estimates and use to bootstrap:\\
		use the value of the best future action
		\item Recall SARSA:
		$$ Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha((r_t + \gamma Q(s_{t+1}, a_{t+1})) - Q(s_t, a_t))$$
		\item Q-Learning:
		$$Q(s_t, a_t) \gets Q(s_t, a_t) + \alpha((r_t + \gamma \max_{a' \in A}Q(s_{t+1},a')) - Q(s_t, a_t)) $$
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Q-Learning with $\epsilon$-greedy Exploration}
	
	\begin{itemize}
		\item Initialization:
		\begin{itemize}
			\item $Q(s,a)=0. \forall s \in S, a \in A$
			\item $t=0$
			\item initial state $s_t = s_0$
		\end{itemize} 
		\item Loop
		\begin{itemize}
			\item Take action $a_{t} \sim \pi_b(s_{t})$ 
			\item Observe $(r_{t}, s_{t+1})$
			\item Update Q 
			$$Q(s_t,a_t) \gets Q(s_t, a_t) + \alpha (r_t + \gamma \max_{a\in A} Q(s_{t+1}, a) - Q(s_t, a_t))$$
			\item $\pi_b(s_t) \in \argmax_{a\in A} Q(s_t, a)$ with probability $1-\epsilon$, else random
			\item $t = t+1$
		\end{itemize} 
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Q-Learning with $\epsilon$-greedy Exploration}
	
	\begin{itemize}
		\item Conditions for convergence to $Q^\pi$?
		\begin{itemize}
			\item Visit all $(s, a)$ pairs infinitely often
			\item the step-sizes $\alpha_t$ satisfy the Robbins-Munro sequence
			\item Note: the algorithm does not have to be greedy in the limit of infinite exploration (GLIE) to satisfy this
		\end{itemize}
		\bigskip
		\item Conditions for convergence to optimal $\pi^*$
		\begin{itemize}
			\item The above requirements to converge to optimal $Q^*$
			\item The algorithm is GLIE
		\end{itemize}
	\end{itemize}
	
\end{frame}
%--------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
