% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Meta-RL]{Meta Reinforcement Learning}
\subtitle{Formalization of Meta-RL}
\usepackage{todonotes}

\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{RL Problem Setting}

\begin{itemize}
	\item Definition of MDP $(S,A,P, R, \gamma)$
	\begin{itemize}
		\item $S$ is a (finite) set of Markov states $s \in S$
		\item $A$ is a (finite) set of actions $a \in A$
		\item $P$ is dynamics/transition model for each action, that specifies $P(s_{t+1} = s' \mid s_t=s, a_t=a)$
		\item $R$ is a reward function 
		$R(s_t=s, a_t=a) = \mathbb{E}[r_r \mid s_t=s, a_t=a] $
		\begin{itemize}
			\item Sometimes R is also defined based on $(s)$ or on $(s,a,s')$
		\end{itemize}
		\item Discount factor $\gamma \in [0, 1]$
	\end{itemize}
	\bigskip
	\item Task: Compute the optimal policy
		$$ \pi^*(s)  \in \argmax_\pi V^\pi(s)$$
\end{itemize}


\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Meta-RL Problem Setting}
	
\begin{itemize}
	\item No single formal problem setting
	\item Usually based on a set of different MDPs $\mathcal{M}$ 
	\item Commonalities in $\mathcal{M}$ vary
	\item Often: shared state \& action space
	\item Task: Compute optimal policy over all of $\mathcal{M}$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------

\todo[inline]{Format this better for align with the speaking style}

\begin{frame}[c]{Contextual MDPs}
	
One way of formalizing Meta-RL are \emph{cMDPs}~\lit{Hallak et al., 2015}{https://arxiv.org/pdf/1502.02259.pdf}:
\begin{itemize}
    \item A cMDP $\mathcal{M}_{\mathcal{I}}$ is a set of MDPs: $\mathcal{M}_{\mathcal{I}} := \{M_i | i \in \mathcal{I}\}$
    \item $\mathcal{S}$ and $\mathcal{A}$ are shared between all of them
    \item $\mathcal{R}$ and $\mathcal{T}$ can vary between MDPs
    \item $\mathcal{I}$ is a set of \emph{contexts}
    \item The context $c_i$ decides how $\mathcal{R}_i$ and $\mathcal{T}_i$ look for $\mathcal{M}_i \in  \mathcal{M}_{\mathcal{I}}$
    \item Task in cMDPs: Compute the optimal policy over $\mathcal{I}$
		$$ \forall c_i \in \mathcal{I}: \pi^*(s, c_i)  \in \argmax_\pi V^\pi(s, c_i)$$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------

\todo[inline ]{Not required here -- talk about the general picture at first}

\begin{frame}[c]{In Practice: CARL~\lit{Benjamins et al. 2021}{https://arxiv.org/pdf/2110.02102.pdf}}

\begin{figure}
    \centering
    \includegraphics[width=0.75\textwidth]{w09_meta_rl/images/concept}
    \caption{Contextual RL in CARL}
    \label{fig:my_label}
\end{figure}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------

\end{document}
