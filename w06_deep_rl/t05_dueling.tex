% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Deep Reinforcement Learning]{RL: Deep}
\subtitle{Dueling Networks}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Value \& Advantage Function}
	

\begin{itemize}
	\item Intuition: Different features might be needed either (a) to accurately represent value or\\ (b) to specify differences in actions
	\item For example
	\begin{itemize}
		\item Game score may help accurately predict $V(s)$
		\item But not necessarily in indicating relative action values $Q(s,a_1)$ vs $Q(s,a_2)$
	\end{itemize}
	\item Advantage function \lit{Baird 1993}\\
	$$A^\pi (s,a) = Q^\pi(s,a) - V^\pi(s) $$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dueling DQN~\lit{Wang et al. 2016}{https://arxiv.org/abs/1511.06581}}

\begin{center}
Standard DQN

\vspace{-2em}
\includegraphics[width=0.38\textwidth]{images/dueling_networks.png}

Plain DQN (above) vs Dueling DQN (below)
\end{center}

\begin{itemize}
	\item Above head predicts $V(s)$
	\item Heads below predicts $A(s,a_1)$, $A(s,a_2)$, $\ldots$
	\item Combination: $Q(s,a_1)$, $Q(s,a_2)$, $\ldots$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dueling DQN~\lit{Wang et al. 2016}{https://arxiv.org/abs/1511.06581}}
	
	\begin{itemize}
	\item Advantage function \lit{Baird 1993}\\
	$$A^\pi (s,a) = Q^\pi(s,a) - V^\pi(s) $$
	\item Consider a network that outputs $V(s; \vect{w}_1, \vect{w}_2)$ as well as advantage $A(s,a; \vect{w}_1, \vect{w}_3)$ where $\vect{w}_i$ are the weights of the different parts of the network
	\item To construct $Q$ could use\\ $$Q(s,a;\vect{w}_1, \vect{w}_2, \vect{w}_3) = V(s;\vect{w}_1, \vect{w}_2) + A(s,a;\vect{w}_1, \vect{w}_3)$$
	%\item Do we expect that this architecture will result in us learning a good estimate of true $V$ or $A$?
	\bigskip
	\pause
	\item Challenge: There doesn't have to be a unique advantage function

	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Uniqueness}
	
	\begin{itemize}
		\item Consider a network that outputs $V(s;\vect{w}_1, \vect{w}_2)$ as well as advantage $A(s,a; \vect{w}_1, \vect{w}_3)$
		\item To construct $Q$, could use $$Q(s,a;\vect{w}_1, \vect{w}_2, \vect{w}_3) = V(s;\vect{w}_1, \vect{w}_2) + A(s,a;\vect{w}_1, \vect{w}_3)$$
		\item Option 1: Force $Q(s,a) = V(s)$ for the best action suggested by the advantage:\\
		$$\hat{Q}(s,a;\vect{w}) = \hat{V}(s;\vect{w}) + \left( \hat{A}(s,a;\vect{w}) - \max_{a' \in \mathcal{A}} \hat{A}(s,a';\vect{w}) \right) $$
		\vspace{-1em}
		\begin{itemize}
			\item This helps to force the $V$ network to approximate $V$
		\end{itemize}
		\item Option 2: Use mean as baseline ($\leadsto$ more stable)\\
		$$\hat{Q}(s,a;\vect{w}) = \hat{V}(s;\vect{w}) + \left( \hat{A}(s,a;\vect{w}) - \frac{1}{|\mathcal{A}|} \sum_{a' \in \mathcal{A}} \hat{A}(s,a';\vect{w}) \right) $$
		\vspace{-1em}
		\begin{itemize}
			\item More stable often because averaging over all advantages instead of
			the advantage of the current max action.
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------

\begin{frame}[c]{Putting it all Together -- RAINBOW ~\lit{Hessel et. al.}{https://arxiv.org/abs/1710.02298}}
	
	\begin{itemize}
		\item Previously we introduced several additions to the DQN architecture separately
        \item \textbf{Idea:} What happens if we combine them? 
        \item RAINBOW is an architecture that combines these techniques (with some other ideas that can be found in the paper) to demonstrate superior performance on the Atari Benchmark 
	\end{itemize}
	
\end{frame}

%-----------------------------------------------------------------------
%-----------------------------------------------------------------------

\begin{frame}[c]{Putting it all Together -- RAINBOW ~\lit{Hessel et. al.}{https://arxiv.org/abs/1710.02298}}
	
	\begin{figure}
	    \centering
	    \includegraphics[width=.4\linewidth]{w06_deep_rl/images/rainbow.png}
	\end{figure}
	
\end{frame}

\end{document}
