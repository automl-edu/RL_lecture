% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Deep Reinforcement Learning]{RL: Deep}
\subtitle{Dueling Networks}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Value \& Advantage Function}
	

\begin{itemize}
	\item Intuition: Different features might be needed either (a) to accurately represent value or\\ (b) to specify difference in actions
	\item For example
	\begin{itemize}
		\item Game score may help accurately predict $V(s)$
		\item But not necessarily in indicating relative action values $Q(s,a_1)$ vs $Q(s,a_2)$
	\end{itemize}
	\item Advantage function \lit{Baird 1993}\\
	$$A^\pi (s,a) = Q^\pi(s,a) - V^\pi(s) $$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dueling DQN~\lit{Wang et al. 2016}{https://arxiv.org/abs/1511.06581}}

\begin{center}
Standard DQN

\vspace{-2em}
\includegraphics[width=0.38\textwidth]{images/dueling_networks.png}

Plain DQN (above) vs Dueling DQN (below)
\end{center}

\begin{itemize}
	\item Above head predicts $V(s)$
	\item Heads below predicts $A(s,a_1)$, $A(s,a_2)$, $\ldots$
	\item Combination: $Q(s,a_1)$, $Q(s,a_2)$, $\ldots$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dueling DQN~\lit{Wang et al. 2016}{https://arxiv.org/abs/1511.06581}}
	
	\begin{itemize}
	\item Advantage function \lit{Baird 1993}\\
	$$A^\pi (s,a) = Q^\pi(s,a) - V^\pi(s) $$
	\item Consider a network that outputs $V(s; \vect{w}_1, \vect{w}_2)$ as well as advantage $A(s,a; \vect{w}_1, \vect{w}_3)$ where $\vect{w}_i$ are the weights of the different parts of the network
	\item To construct $Q$ could use\\ $$Q(s,a;\vect{w}_1, \vect{w}_2, \vect{w}_3) = V(s;\vect{w}_1, \vect{w}_2) + A(s,a;\vect{w}_1, \vect{w}_3)$$
	%\item Do we expect that this architecture will result in us learning a good estimate of true $V$ or $A$?
	\bigskip
	\pause
	\item Challenge: There doesn't have to be a unique advantage function

	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Uniqueness}
	
	\begin{itemize}
		\item Consider a network that outputs $V(s;\vect{w}_1, \vect{w}_2)$ as well as advantage $A(s,a; \vect{w}_1, \vect{w}_3)$
		\item To construct $Q$, could use $$Q(s,a;\vect{w}_1, \vect{w}_2, \vect{w}_3) = V(s;\vect{w}_1, \vect{w}_2) + A(s,a;\vect{w}_1, \vect{w}_3)$$
		\item Option 1: Force $Q(s,a) = V(s)$ for the best action suggested by the advantage:\\
		$$\hat{Q}(s,a;\vect{w}) = \hat{V}(s;\vect{w}) + \left( \hat{A}(s,a;\vect{w}) - \max_{a' \in \mathcal{A}} \hat{A}(s,a';\vect{w}) \right) $$
		\vspace{-1em}
		\begin{itemize}
			\item This helps to force the $V$ network to approximate $V$
		\end{itemize}
		\item Option 2: Use mean as baseline ($\leadsto$ more stable)\\
		$$\hat{Q}(s,a;\vect{w}) = \hat{V}(s;\vect{w}) + \left( \hat{A}(s,a;\vect{w}) - \frac{1}{|\mathcal{A}|} \sum_{a' \in \mathcal{A}} \hat{A}(s,a';\vect{w}) \right) $$
		\vspace{-1em}
		\begin{itemize}
			\item More stable often because averaging over all advantages instead of
			the advantage of the current max action.
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
