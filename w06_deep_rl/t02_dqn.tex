% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Deep Reinforcement Learning]{RL: Deep}
\subtitle{DQN}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{RL with Function Approximation}
	
\begin{itemize}
	\item Represent state-action value function by $Q$-network with weights $\vect{w}$
	$$\hat{Q}(s,a;\vect{w}) \approx Q(s,a)$$
\end{itemize}

\centering
\includegraphics[width=0.7\textwidth]{../w05_function_approx/images/vfa.png}


\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recall: Incremental Model-Free Control Approaches}
	
\begin{itemize}
	\item Similar to policy evaluation, the true state-action value function for a state is unknown and so substitute a target value
	\item In Monte Carlo methods, use a return $G_t$ as a substitute target
	$$\Delta \vect{w} = \alpha(G_t - \hat{Q}(s_t,a_t; \vect{w})) \nabla_{\vect{w}} \hat{Q}(s_t, a_t; \vect{w}) $$
	\item For SARSA instead use a TD target $r+ \gamma \hat{Q}(s', a'; \vect{w})$ which leverages the current function approximations value
	$$\Delta \vect{w} = \alpha (r + \gamma \hat{Q}(s',a';\vect{w}) - \hat{Q}(s,a;\vect{w})) \nabla_{\vect{w}}\hat{Q}(s,a;\vect{w}) $$
	\item For Q-learning instead use a TD target $r + \gamma \max_{a'} \hat{Q}(s',a';\vect{w})$ which leverages the max of the current function approximations value
	$$\Delta \vect{w} = \alpha (r + \gamma \max_{a'} \hat{Q}(s',a';\vect{w}) - \hat{Q}(s,a;\vect{w})) \nabla_{\vect{w}}\hat{Q}(s,a;\vect{w}) $$
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Using these Ideas to do Deep RL in Atari}
	
\centering
\includegraphics[width=0.5\textwidth]{images/atari_deep_rl.png}

\begin{flushright}
	\small
	Image by David Silver
\end{flushright}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Using these Ideas to do Deep RL in Atari}
	
\begin{itemize}
	\item End-to-end learning of values $Q(s, a)$ from pixels $s$
	\item Input state $s$ is stack of raw pixels from last $4$ frames
	\item Output is $Q(s, a)$ for $18$ joystick/button positions
	\item Reward is changed in score for that step
	\item Network architecture and hyperparameters fixed across all games
\end{itemize}

\centering
\includegraphics[width=0.6\textwidth]{images/atari_dqn_arch.png}

\begin{flushright}
	\footnotesize
	\vspace{-0.1cm}
	DQN source code: \url{sites.google.com/a/deepmind.com/dqn/}
\end{flushright}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Q-Learning with Value Function Approximation}
	
	\begin{itemize}
		\item Minimize MSE loss by stochastic gradient descent
		\item Converges to the optimal $Q^*(s,a)$ using \alert{table lookup} representation
		\item But Q-learning with VFA can diverge
		\item Two of the issues causing problems:
		\begin{itemize}
			\item Correlations between samples violates i.i.d assumption of DNNs
			\item Non-stationary targets
		\end{itemize}
		\item Deep Q-learning (DQN) addresses both of these challenges by
		\begin{itemize}
			\item Experience replay
			\item Fixed Q-targets
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{DQNs: Replay Buffer}
	
	\begin{itemize}
		\item To help remove correlations, store dataset (called a \alert{replay buffer}) $\mathcal{D}$ from prior experience
		\item To perform experience replay, repeat the following:
		\begin{enumerate}
			\item $(s,a,r,s')\sim \mathcal{D}$: sample experience tuple from the dataset
			\item Compute the target value for the sampled $s$: $r+\gamma \max_{a'} \hat{Q}(s',a';\vect{w})$
			\item Use stochastic gradient descent to update the network weights
			$$\Delta \vect{w} = \alpha (r + \gamma \max_{a'} \hat{Q}(s',a';\vect{w}) - \hat{Q}(s,a;\vect{w})) \nabla_{\vect{w}}\hat{Q}(s,a;\vect{w})$$
		\end{enumerate}
		\pause
		\item Remarks:
		\begin{itemize}
			\item Fixed sized buffer $\leadsto$ first-in--first-out scheme (as default implementation)
			\item heuristic trade-off between performing new episodes and sampling from the replay buffer
		\end{itemize}
			
		\medskip
		\pause
		\item[$\leadsto$] Can treat the target as a scalar, but the weights will get
		updated on the next round, changing the target value
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{DQNs: Fixed Q-Targets}
	\vspace{-1em}
	\begin{itemize}
		\item To help improve stability, fix the \alert{target weights} used in the target calculation for multiple updates
		\item Target network uses a different set of weights than the weights being updated
		\item Let parameters $\vect{w}^-$ be the set of weights used in the target and $\vect{w}$ be the weights that are being updated
		\item Slight change to the computation of target value:
		\begin{itemize}
			\item $(s,a,r,s')\sim \mathcal{D}$: sample experience tuple from the dataset
			\item Compute the target value for the sampled $s$: $r+\gamma \max_{a'} \hat{Q}(s',a';\vect{w}^-)$
			\item Use stochastic gradient descent to update the network weights
			$$\Delta \vect{w} = \alpha (r + \gamma \max_{a'} \hat{Q}(s',a';\vect{w}^-) - \hat{Q}(s,a;\vect{w})) \nabla_{\vect{w}}\hat{Q}(s,a;\vect{w})$$
		\end{itemize}
		\smallskip 
		\pause
		\item Remark:
		\begin{itemize}
			\item Hyperparameter how often you update $\vect{w}^-$
			\item Trade-off between updating too often ($\leadsto$ instability) and\\ too rarely ($\leadsto$ too old state information)
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{DQN Summary}
	
	\begin{itemize}
		\item DQN uses experience replay and fixed Q-targets
		\item Store transition $(s_t, a_t, r_{t+1}, s_{t+1})$ in replay memory $\mathcal{D}$
		\item Sample random mini-batch of transitions $(s,a,r,s')$ from $\mathcal{D}$
		\item Compute Q-learning targets wrt old, fixed parameters $\vect{w}^-$
		\begin{itemize}
			\item Update $\vect{w}^-$ from time to time
		\end{itemize}
		\item Optimizes MSE between Q-network and Q-learning targets
		\item Uses stochastic gradient descent
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
