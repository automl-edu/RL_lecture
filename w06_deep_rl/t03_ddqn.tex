% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Deep Reinforcement Learning]{RL: Deep}
\subtitle{Double DQN}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recall: Double Q-Learning}

    
 
	\begin{algorithm}[H]
        \caption{Double Q-Learning}
        \DontPrintSemicolon
        \LinesNotNumbered
        \KwIn{{}
        $s_o$ - start state, $\alpha$ - learning rate\\}
        
        \textbf{Initialise}: $Q_{1}(s,a) \leftarrow \quad  Q_{2}(s,a) \leftarrow\quad \forall s \in S, a \in A$, $t \gets 0$
    
        \While{Not converged}{
            {Take action $a_t \in \argmax Q_1(s_t,a) + Q_2(s_t,a)$}, Observe ($r_t$, $s_{t+1}$)\\
            $x \sim U(0,1)$\\
            \If {$x \leq 0.5$}{
                $Q_1(s_t, a_t) \gets Q_1(s_t, a_t) + \alpha (r_t +\gamma Q_2(s_{t+1}, \argmax_{a \in A} Q_1(s_{t+1},a) ) - Q_1(s_t, a_t))$
            }
            \Else{
                $Q_2(s_t, a_t) \gets Q_2(s_t, a_t) + \alpha (r_t +\gamma Q_1(s_{t+1}, \argmax_{a \in A} Q_2(s_{t+1},a) ) - Q_2(s_t, a_t))$
            }
            $t \gets t+1$
        }    
    \end{algorithm}
    \vspace{-10pt}
	\begin{itemize}
		\pause
		\item[$\leadsto$] reduces maximization bias by determining the change in $Q_1$ based on $Q_2$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Double DQN~\lit{Hasselt et al. 2015}{https://arxiv.org/pdf/1509.06461.pdf}}
	
	\begin{itemize}
		\item Extend this idea to DQN
		\item Current Q-network $\vect{w}$ is used to select actions 
		\item Older Q-network $\vect{w}^-$ is used to evaluate actions
		\item TD-error:
		$$r + \gamma \overbrace{\hat{Q}(s', \underbrace{\argmax_{a' \in A} \hat{Q}(s',a';\vect{w})}_{\text{Action selection: }\vect{w}};\vect{w}^-)}^{\text{Action evaluation: }\vect{w}^-} - Q(s,a;\vect{w})$$
		
		\pause
		\item Allows flipping between both weights sets frequently 
		\begin{itemize}
			\item alternatively, Polyak averaging:
					$$ w^- \gets \tau w + (1 - \tau)w^- $$
			\item $\tau$ is fairly small, e.g, $0.01$
		\end{itemize}
		\item Faster propagation of information compared to original DQN
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Clipped Double DQN~\lit{Fujimoto et al. 2018}{https://arxiv.org/pdf/1802.09477.pdf}}
	
	\begin{itemize}
		\item Extend this idea to DQN
		\item Again having two independent Q-networks with $\vect{w}_1$ and $\vect{w}_2$
		\item Take minimum action value for the successor state
		\item TD-error:
		$$r + \gamma \min_{i=\{1,2\}}Q(s', \argmax_{a' \in A} Q(s', a'; \vect{w}); \vect{w}_i) - Q(s,a;\vect{w})$$
		\begin{itemize}
		\item Less overestimation of Q-values
		\item More stable learning targets
		\end{itemize}
	\end{itemize}
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
