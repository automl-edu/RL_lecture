\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{pgf-pie} 
\newcommand{\todo}[1]{
	% Die folgende Zeile kann auskommentiert werden, um die ToDo's zu entfernen
	\textcolor{red}{[\textbf{ToDo}: \emph{#1}]}
}

\title[Conclusion]{RL: A recipe for RL experiments}
\subtitle{}

%\institute{}


\begin{document}
	
	\maketitle

%-----------------------------------------------------------------------------------------------------------------------------

% \begin{frame}[c]{Basics of RL experimentation}
%     \begin{itemize}
%         \item Seeding
%         \item Plotting and visualization
%         \item Evaluation Methods
%         \item Hyperparameter Optimization
%     \end{itemize}
        
% \end{frame}

%-----------------------------------------------------------------------------------------------------------------------------

\begin{frame}[c]{Experimental Pipeline}

\begin{itemize}
    \item \textbf{Training:} Repeat this process for a predefined number of environment steps after initializing the policy
        \begin{enumerate}
            \item Run episodes to collect experiences
            \item Store them in a buffer (Replay buffer for DQN style agents, rollout buffer for Policy Gradients)
            \item Use the stored experiences for optimization
        \end{enumerate}
    \vfill 
    \item \textbf{Testing:} Run an agent for $N$ test episodes in the environment
        \begin{itemize}
            \item best: keep all returns to get a feeling for the distribution of returns (boxplot, violin plot, histogram)
            \item easiest: calculate average return as an aggregation strategy
        \end{itemize}

    \vfill
    \item \textbf{Key:} Use a separate environment for testing (e.g., a copy of the training environment) and keep testing the agent at a designated number of steps. Example:
    \begin{itemize}
        \item Environment is CartPole, we train with 100k environment interactions (steps)
        \item Every 10k steps we evaluate for 5 episodes on a fresh initialization of the environment
        \item[!] Balance training and evaluation based on the episode length. 
    \end{itemize}
\end{itemize}


        
\end{frame}


%-----------------------------------------------------------------------------------------------------------------------------

\begin{frame}[c]{Seeding}

    \begin{itemize}
        \item  Sources of randomness
            \begin{enumerate}
                \item Initialization of Neural Networks
                \item Initial State distribution
                \item Stochasticity in the policy
                \item Stochasticity in the environment
            \end{enumerate}
        \vfill
        \item  Seeds control the pseudo-random sequences generated by the Random Number Generators (RNGs) and setting the seed manually helps with controlling stochasticity from the agent side of things
        \vfill
        \item  \textbf{Key:} Run your experiments on multiple seeds and record the reward curves across each seed
    \end{itemize}

   
        
\end{frame}

%-----------------------------------------------------------------------------------------------------------------------------

\begin{frame}[c]{Plotting}

    \small{
    Generally, a plotted learning curve consists of the following
    \begin{itemize}
         \item Mean reward across seeds
        \item Confidence interval of $95\%$ \tiny{(beware, depending on the plotting library there are different defaults, e.g., also standard deviation)}
        
    \end{itemize}
    }

    \hfill
   
    \begin{figure}
        \centering
        \includegraphics[width=0.4\textwidth]{w11_wrap_up/images/Example_Image.png}
        \caption{Example Image for Plotting. Mean and $95\%$ CIs with 10 seeds.}
        \label{fig:my_label}
    \end{figure}

    
\end{frame}

%-----------------------------------------------------------------------------------------------------------------------------

\begin{frame}[c]{Plotting II}

    Tips
    \begin{itemize}
        \item Describe what you plot, e.g., in the image caption
        \item You can use \texttt{seaborn} as a plotting library (there are more) (if you use \texttt{seaborn} check out \texttt{FacetGrid} 
        \item Font sizes should be large enough
        \item Use colorblind palettes if possible
    \end{itemize}

    
\end{frame}

%-----------------------------------------------------------------------------------------------------------------------------

\begin{frame}[c]{Evaluating a New Algorithm}

    We generally present results in two manners 
    \vfill
    \begin{itemize}
        \item  \textbf{Baselines:} How much does my algorithm improve upon what is already done?
            \begin{enumerate}
                \item \textbf{Random Agent:} Comparing against an agent that randomly takes actions to show that our agent has learned something 
                \item \textbf{Heuristic agent:} an agent thatâ€™s using a simple action choice heuristic that strongly depends on the nature of the problem itself
                \item Other RL agents that follow approaches similar to our approach
            \end{enumerate}
        \vfill
        \item  \textbf{Ablations:} What happens if I remove one design choice from my agent?
            \begin{enumerate}
                \item Ideal for approaches that are so new that baselines might be difficult 
                \item Allows understanding of how different components behave
            \end{enumerate}
        
    \end{itemize}

\end{frame}

%-----------------------------------------------------------------------------------------------------------------------------

\begin{frame}[c]{Evaluating a New Environment}

    \begin{itemize}
        \item We recommend using the gym interface (version 0.21.0) for environments (so you can use standard agents, e.g., from stable baselines 3)
        \vfill
        \item The environment should maintain a state, and accept an action
        \vfill
        \item Given an action from the agent, the environment returns 
            \begin{enumerate}
                \item Next state
                \item Reward
                \item A boolean flag indicating whether the episode terminated or not
				\item An info dict (most of the time it is empty)
            \end{enumerate}
        \vfill
        \item \textbf{Key:} 
        \begin{enumerate}
            \item Carefully think about each design component! The sparsity of the reward and information presented to the agent by the state can have a big impact on learning progress
            \item Test the environment with simple heuristically driven agents for corner cases
        \end{enumerate}
    \end{itemize}

\end{frame}

%-----------------------------------------------------------------------------------------------------------------------------

\begin{frame}[c]{Misc Tips}

    \begin{itemize}   
        \item Experimental Setup:
            \begin{enumerate}
                \item Automate running of experiments to have quick results 
                \item Maintain configurations of agents and environments in configuration Files (E.g. Hydra: \url{https://hydra.cc/docs/intro/})
                \item Benchmark your code and package it for better reproducibility
                \item Have a good experimental setup for tracking learning progress (E.g. Weights and Biases: \url{https://wandb.ai/site}). 
            \end{enumerate}
        \item RL agent Diagnostics
            \begin{enumerate}
                \item Benchmark your agent/environment as much as possible
                \item Standardize your data if the ranges of states/observations are unknown. Do the same with targets (E.g. value estimates)
                \item \textbf{Important HPs:} $\gamma$, $\lambda$, $\alpha$, $\epsilon$ ... 
                \item \textbf{Diagnostic tools:} State visitation frequency, KL divergence between old and new policies. 
            \end{enumerate}
        
    \end{itemize}

\end{frame}





\end{document}