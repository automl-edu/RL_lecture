% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Exploration]{Exploration in RL}
\subtitle{Traditional Exploration Strategies for Bandits}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recap: The Bandit Problem}

\begin{itemize}
	\item Simplified RL setting with no states
	\item Simply try to identify which action $a^* \in \mathcal{A}$ is the best one
	\begin{itemize}
		\item of course, we want to be efficient in doing that!
	\end{itemize}
	\item Reward is drawn from some unknown distribution
	\bigskip
	\pause
	\item[$\leadsto$] That's exactly the problem you face in every state $s$ again.\\ Let's assume that we fix $s$ for the moment
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Optimistic Initialization}
	
	\begin{itemize}
		\item Simple idea: initialize $\hat{Q}_0(a)$ to high values
		\item Update action value incrementally
		\item Starting with $N(a) > 0$
		$$\hat{Q}_t(a_t) = \hat{Q}_{t-1}(a_t) + \frac{1}{N_t(a_t)} (r_t - \hat{Q}_{t-1}(a_t))$$
		\item Encourages systematic exploration early on
		\item But can still lock onto suboptimal action
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{$\epsilon$-greedy}
	
	\begin{itemize}
		\item play best known action $\hat{a}$ with probability $1-\epsilon$
		\item play a random action $a \in \mathcal{A}$ with probability $\epsilon$ 
		\smallskip
		\item Question: Is this a zero-regret strategy for a constant $\epsilon$?
		\pause
		\begin{itemize}
			\item No, since with probability $\epsilon$ we will obtain a non-zero regret and therefore, even in the limit the regret will not go to zero, but obtain a linear regret.
		\end{itemize}
		\smallskip\pause
		\item Solution: Anneal $\epsilon$ over time
		\item If we linearly anneal $\epsilon$ over time to $0$, do we have a zero-regret strategy?
		\pause
		\begin{itemize}
			\item No, because of the linear annealing, we have only a finite amount of observations which might not suffice to identify the best action
		\end{itemize}
		\smallskip
		\item[$\leadsto$] Anneal $\epsilon$ proportional $\sqrt{t}$ or $1/t$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Upper Confidence Bounds}
	
	\begin{itemize}
		\item Track all rewards you obtained by playing each action $a_k$ and compute mean $\mu(a_k)$ and standard deviation $\sigma(a_k)$ to estimate the underlying reward distribution
		\item "Optimistic in face of uncertainty" by upper confidence bound:
		$$\mu(a_k) + \kappa \cdot \sigma(a_k) / \sqrt{N(a_k)}$$
		\item Idea: Over time, we get more and more evidence for the best actions until we are sure that the best known is really the best
		\medskip
		\pause
		\item To prevent premature convergence: Use optimistic initialization of each action\\ s.t. all actions are played in the beginning
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{UCB~1}
	
			$$a_t \in \argmax_{a \in A} Q(a) + \sqrt{\frac{2\log t }{N_t(a)}} $$
	
	\begin{itemize}

		\item Condition: Rewards have to be i.i.d random variables in $[0,1]$.
		\pause
		\item Theorem: The UCB~1 algorithm achieves logarithmic asymptotic total regret
		$$\lim_{t\to \infty} L_t \leq 8 \log t \sum_{a|\Delta_a>0} \Delta_a $$
		\begin{itemize}
			\item where $L_t$ is the regret after $t$ trials and $\Delta_a = V^* - Q(a)$
			\item Using Hoeffding's Inequality
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Thompson Sampling}
	
	\begin{itemize}
		\item Track all rewards you obtained by playing each action $a_k$ and compute mean $\mu(a_k)$ and standard deviation $\sigma(a_k)$ to estimate the underlying distribution
		\item Draw from each estimated distribution a single realization and simply play the action with the best one, e.g.
		$$ s_k \sim \mathcal{N}(\mu(a_k), \sigma(a_k))$$
		$$ a \in \argmax_{a_k} s_k$$
		\item In the limit, only the best performing action will be played with high probability
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
