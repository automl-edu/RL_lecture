\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}

\title[ML-RL: Big Picture]{RL: Introduction}
\subtitle{In a Nutshell}



\begin{document}
	
\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Components of RL Problems}
	
	\centering
	\includegraphics[width=0.55\textwidth]{images/rl_comic.png}\footnote{Image source: Morning Brew and Marius Haakestad on Unsplash}
	
	\bigskip
	
	\begin{itemize}
		\item Data: Self-acquired observations + rewards
		\item Task: Learn how to behave s.t. reward is maximized
	\end{itemize}	
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{State?}
	
	\begin{itemize}
		\item We constantly observe our environment (and our own state)
		\item Mostly via sensors
		\begin{itemize}
			\item images
			\item sound
			\item feeling by touch
			\item feeling of acceleration
			\item feeling of balance
			\item ...
		\end{itemize}
		\pause
		\smallskip
		\item Sometimes we are also presented by explicit information from our env
		\begin{itemize}
			\item Documents
			\item Scores
			\item ...
		\end{itemize}
		\pause
		\smallskip
		\item[$\leadsto$] We never observe the full state, but only an abstraction of it
		\item[$\leadsto$] some distinguish between states $s$ and observations $o$
	\end{itemize}
	
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Actions}
	
	\begin{itemize}
		\item In a given state, an action will (potentially) change the state 
		\medskip
		\pause
		\item Types of actions:
		\begin{description}
			\item[continuous] The value domain is continuous and often bounded by some range (e.g., $[0,1]$)
			\begin{itemize}
				\item Examples: velocity, angles, probabilities
			\end{itemize}
			\pause
			\item[categorical and discrete] The action is to choose from a set of possible options (i.e., potentially no ordering between actions)
			\begin{itemize}
				\item Examples: button on a game controller, set of strategies, discrete position on a board
			\end{itemize}
		\end{description}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------

%----------------------------------------------------------------------
\begin{frame}[c]{Transitions}
	
	\begin{itemize}
		\item Given state $s$ and action $a$, in which state do we end up?
		\smallskip
		\pause
		\item Either deterministic: We will end up exactly in one state
		\begin{itemize}
			\item Examples: board games like Go or Chess
		\end{itemize}
		\pause
		\item Or non-deterministic: There is probability distribution over\newline in which states we will end up.
		\begin{itemize}
			\item Examples: games with randomized events (e.g., many card games), robotics -- often because the control over our robot is not perfect
		\end{itemize}
		\item Challenges:
		\begin{itemize}
			\item Was the action responsible for the stochasticity or the environment?
			\item Harder to learn in such an environment since you have a different notion of reproducibility
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Rewards}
	\begin{itemize}
		\item Feedback on whether we did something "good" or "bad"
		\smallskip
		\pause
		\item Either immediate (or dense) reward: We directly get a reward signal after each transition
		\item Or delayed (or sparse) reward: We have to wait some states to observe the reward
		\begin{itemize}
			\item Examples: Saving for retirement or Finding a key in video game Montezumaâ€™s revenge
			\item Extreme case: we get only feedback at the end of an episode\newline (e.g., who won a board game match)
		\end{itemize}
        \item Either deterministic or stochastic (focus on deterministic)
        \pause
		\item Introduces two challenges
		\begin{itemize}
			\item When planning: decisions involve reasoning about not just immediate
			the benefit of a decision but also its longer-term ramifications
			\item When learning: temporal credit assignment is hard (what caused later
			high or low rewards?)
		\end{itemize}
	\end{itemize}
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Episode}
	\begin{itemize}
		\item An episode is a sequence of state-action(-reward) pairs (i.e. steps)
		\item The end of an episode is called a horizon
		\smallskip
		\pause
		\item Finite horizon: We have a finite amount of steps\newline until the episode ends
		\item Infinite horizon: The episode will never end (unless we abort it)
	\end{itemize}
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
