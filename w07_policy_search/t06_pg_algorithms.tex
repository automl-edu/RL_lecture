% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Policy Gradient]{RL: Policy Search}
\subtitle{Policy Gradient Algorithms}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Likelihood Ratio / Score Function Policy Gradient}
	
	
	$$\nabla_\theta V(\theta) = \frac{1}{m} \sum_{i=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta (a_t^{(i)} \mid s_t^{(i)}) $$
	
	\begin{itemize}
		\item Unbiased but very noisy
		\item Fixes that can make it practical
		\begin{itemize}
			\item Temporal structure
			\item Baseline
			\item (and some more)
		\end{itemize}
		
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Gradient: Use Temporal Structure}
	
$$ \nabla_\theta \mathbb{E}_\tau [R] = \mathbb{E}_\tau \left[ \left(\sum_{t=0}^{T-1} r_t\right) \left( \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t \mid s_t) \right) \right]$$
	
We can repeat the same argument to derive the gradient estimator for a single reward term $r_{t'}$

$$\nabla_\theta \mathbb{E} [r_{t'}] = \mathbb{E} \left[r_{t'} \sum_{t=0}^{t'} \nabla_\theta \log \pi_\theta (a_t \mid s_t)\right] $$
	
Summing this formula over $t$, we obtain:

\begin{eqnarray}
V(\theta) = \nabla_\theta \mathbb{E}[R] &=& \left[ \sum_{t'=0}^{T-1} r_{t'} \sum^{t'}_{t=0} \nabla_\theta \log \pi_\theta (a_t \mid s_t)  \right]\nonumber\\
&=& \left[ \sum_{t=0}^{T-1}  \nabla_\theta \log \pi_\theta (a_t \mid s_t)   \sum^{T-1}_{t=t} r_{t'}  \right]\nonumber
\end{eqnarray}

	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Gradient: Use Temporal Structure}
	
\begin{itemize}
	\item Recall for a particular trajectory $\tau^{(i)}$, $\sum_{t'=t}^{T-1} r_{t'}^{(i)}$ is the return $G_t^{(i)}$
\end{itemize}

$$\nabla_\theta \mathbb{E}[R] \approx \frac{1}{m} \sum_{i=1}^m \sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta (a_t,s_t) G_t^{(i)} $$
	
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Monte-Carlo Policy Gradient (REINFORCE)}
	
Leverages likelihood ratio / score function and temporal structure

$$ \Delta \theta_t = \alpha \nabla_\theta \log \pi_\theta (s_t, a_t) G_t $$
	
	
REINFORCE:
\begin{enumerate}
	\item Initialize policy parameters $\theta$ arbitrarily
	\item for each episode $\{s_1, a_1, r_2, \ldots, s_{T-1}, a_{T-1}, r_T \} \sim \pi_\theta $ do
	\begin{itemize}
		\item for $t=1$ to $T - 1$ do
		\begin{itemize}
			\item $\theta := \theta + \alpha \nabla_\theta \log \pi_\theta (s_t, a_t) G_t $
		\end{itemize}
	\end{itemize}
	\item return $\theta$
\end{enumerate}

	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Gradient: Introduce Baseline}
	
\begin{itemize}
	\item Reduce variance by introducing a baseline $b(s)$
	
	$$\nabla_\theta \mathbb{E}_\tau [R] = \mathbb{E}_\tau \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t \mid s_t; \theta) \left( \sum_{t'=t}^{T-1} r_{t'} - b(s_t) \right) \right] $$
	
	\item For any choice of $b$, gradient estimator is unbiased
	\item Near optimal choice is the expected return
	$$b(s_t) \approx \mathbb{E} [r_t + r_{t+1} +\ldots + r_{T-1}] $$
	
	\item Interpretation: increase logprob of action $a_t$ proportionally to how much returns $\sum_{t'=t}^{T-1} r_{t'}$ are better than expected
	
\end{itemize}
	
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{”Vanilla” Policy Gradient Algorithm}
	
	\begin{itemize}
		\item Initialize policy parameters $\theta$ and baseline $b$
		\item for iteration$=1,2,\ldots$ do
		\begin{itemize}
			\item Collect a set of trajectories by executing the current policy
			\item At each time step $t$ in each trajectory $\tau^i$, compute
			\begin{itemize}
				\item Return $G_t^i= \sum_{t'=t}^{T-1} r_{t'}^i$ and
				\item Advantage estimate $\hat{A}^i_t = G^i_t - b(s_t)$
			\end{itemize}
			\item Re-fit the baseline by minimizing $\sum_i \sum_t || b(s_t) - G^i_t||^2$
			\item Update the policy, using a policy gradient estimate $\hat{g}$
			\begin{itemize}
				\item which is a sum of terms $\nabla_\theta \log \pi(a_t \mid s_t; \theta) \hat{A}_t$
				\item Apply gradient $\hat{g}$ by any DL-optimizer (e.g., SGD or ADAM)
			\end{itemize}
		\end{itemize}
		
	\end{itemize}
	
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Choosing the Baseline: Value Functions}
	
	\begin{itemize}
		\item Recall $Q$-function:
		$$Q^\pi(s,a) = \mathbb{E}_\pi [r_0 +  \gamma r_1 + \gamma^2 r_2 + \ldots \mid s_0 = s, a_0 = a ]$$
		\item State-value function can serve as a great baseline
		\begin{eqnarray}
		V^\pi (s) &=& \mathbb{E}_\pi [r_0 +  \gamma r_1 + \gamma^2 r_2 + \ldots \mid s_0 = s]\nonumber\\	
		&=& \mathbb{E}_{a\sim\pi} [Q^\pi(s,a)]\nonumber
		\end{eqnarray}
		\item Advantage function: Combining $Q$ with baseline $V$:
		$$A^\pi(s,a) = Q^\pi(s,a) - V^\pi(s) $$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
