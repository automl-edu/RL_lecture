% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Policy Gradient]{RL: Policy Search}
\subtitle{Step Size and Trust Region}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Gradient and Step Size}
	
    \begin{itemize}
        \item Goal: Each step of policy gradient yields an updated policy $\pi'$ whose value is greater than (or equal) to the prior policy $\pi$: $V^{\pi'} \geq V^\pi$
        \begin{itemize}
            \item Monotonic improvement
            \item Important in some applications
            \item PG often more stable than DQN
        \end{itemize} 
        \item Gradient descent approaches update the weights a small step in direction of the gradient
        \item First order / linear approximation of the value function's dependence on the policy parameterization
        \item Locally a good approximation; further away less good
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Why are step sizes a big deal in RL?}
	
    \begin{itemize}
        \item Step size is important in any problem involving the optima of a function
        \item Supervised learning: Step too far $\leadsto$ next updates might fix it
        \item Reinforcement learning:
        \begin{itemize}
            \item Step too far $\leadsto$ bad policy
            \item Next batch: collected under bad policy
            \item \alert{Policy is determining data collection!} 
            \begin{itemize}
                \item Essentially controlling exploration and exploitation trade-off due to particular policy parameters and the stochasticity of the policy
            \end{itemize}
            \item May not be able to recover from a bad choice $\leadsto$ collapse in performance
        \end{itemize}
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Gradient Methods with Auto-Step-Size Selection}
	
    \begin{itemize}
        \item Can we automatically ensure the updated policy $\pi'$ has value greater than (or equal to) the prior policy $V^{\pi'} \geq V^\pi$?
        \item Consider this for the policy gradient setting, and hope to address this by modifying step size
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Objective Function}
	
    \begin{itemize}
        \item Goal: find policy parameters that maximize value function
        $$ V(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t); \pi_\theta \right]$$
        \item have access to samples from the current policy $\pi_\theta$ 
        \item Want to predict the value of a different policy ($\leadsto$ off-policy learning)
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Objective Function}
	\vspace{-1em}
    \begin{itemize}
        \item Goal: find policy parameters that maximize value function
        $$ V(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t); \pi_\theta \right]$$
        \item Express value of $\Tilde{\pi}$ in terms of advantage of $\pi$
        \begin{eqnarray}
        V(\Tilde{\theta}) &=& V(\theta) + \mathbb{E}_{\pi_{\Tilde{\theta}}} \left[ \sum_{t=0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right]\\
        &=& V(\theta) + \sum_{s} \mu_{\Tilde{\pi}}(s) \sum_{a} \Tilde{\pi}(a \mid s) A_\pi(s_t, a_t) 
        \end{eqnarray}
        \item $ \mu_{\Tilde{\pi}}(s)$ is the discounted weighted frequency of state $s$ under policy $\Tilde{\pi}$
        \item We know the advantage $A_\pi$ and $\Tilde{\pi}$
        \item But we cannot compute the above we don't know $\mu(\Tilde{\pi})$, the state distribution under the newly proposed policy
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Local Approximation}
	
    \begin{itemize}
        \item Can we remove the dependency on the discounted visitation frequencies under the new policy?
        \item Substitute in the discounted visitation frequency under the current policy to define a new objective function:
        $$ L_\pi(\Tilde{\pi}) = V(\theta) + \sum_{s} \alert{\mu_\pi (s)} \sum_{a} \Tilde{\pi}(a \mid s) A_\pi(s,a)$$
        \item Note that $L_{\pi_{\theta_0}}(\pi_{\theta_0}) = V(\theta_0)$
        \item Gradient of $L$ is identical to gradient of value function at policy parameterized evaluated at $\theta_0$: 
        $$\nabla_{\theta} L_{\pi_{\theta_0}}(\pi_{\theta_0})\mid_{\theta = \theta_0} = \nabla_\theta V(\theta)\mid_{\theta = \theta_0}$$
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Conservative Policy Iteration}
	
    \begin{itemize}
        \item Is there a bound on the performance of a new policy obtained by optimizing the surrogate objective?
        \item Consider mixture policies that blend an old policy and a different policy
        $$\pi_{new}(a \mid s) = (1 - \alpha) \pi_{old} (a \mid s) + \alpha \pi'(a \mid s) $$
        \item In this case, we can guarantee a lower bound on value of the new $\pi_{new}$:
        $$V^{\pi_{new}} \geq L_{\pi_{old}} (\pi_{new}) - \frac{2\epsilon\gamma}{(1- \gamma)^2} \alpha^2 $$
        where $\epsilon = \max_{s} \mathbb{E}_{a \sim \pi'(a \mid s) [A_\pi(s,a)]}$
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Find the Lower-Bound in General Stochastic Policies}
	
	 \vspace{-1.5em}
    \begin{itemize}
        \item Would like to similarly obtain a lower bound on the potential performance for general stochastic policies (not just mixture policies)
        \item Recall $ L_\pi(\Tilde{\pi}) = V(\theta) + \sum_{s} \mu_\pi (s) \sum_{a} \Tilde{\pi}(a \mid s) A_\pi(s,a)$ 
    \end{itemize}
    
    \begin{block}{Theorem}
    Let $D_{TV}^{\max} (\pi_1, \pi_2) = \max_{s} D_{TV}(\pi_1(\cdot \mid s), \pi_2(\cdot\mid s)) = \max_{s} \max_{a} (\pi_1(a\mid s) -\pi_2 (a\mid s))$. Then
    $$V^{\pi_{new}} \geq L_{\pi_{old}} (\pi_{new}) - \frac{4\epsilon\gamma}{(1-\gamma)^2} (D_{TV}^{\max}(\pi_{old}, \pi_{new}))^2$$
    \vspace{-0.3em}
    where $\epsilon = \max_{s,a}|A_\pi(s,a)|$
    \end{block}
    
    \begin{itemize}
        \item Note that $D_{TV}(p,q)^2 \leq D_{KL}(p,q)$ for prob. distribution $p$ and $q$
        \item Then the above theorem immediately implies that
        $$ V^{\pi_{new}} \geq  L_{\pi_{old}} (\pi_{new}) -  \frac{4\epsilon\gamma}{(1-\gamma)^2} D_{KL}^{\max}(\pi_{old}, \pi_{new}) $$
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Optimization of Parameterized Policies}
	
    \begin{itemize}
        \item Goal is to optimize
        $$ L_{\pi_{old}} (\pi_{new}) -  \frac{4\epsilon\gamma}{(1-\gamma)^2} D_{KL}^{\max}(\pi_{old}, \pi_{new}) = L_{\pi_{old}} (\pi_{new}) -  C \cdot D_{KL}^{\max}(\pi_{old}, \pi_{new}) $$
        where $C$ is the penalty coefficient (i.e. hyperparameter)
        \item The penalty (from theory) would be fairly small in practice
        \medskip
        \item New Idea: Use a trust region constraint on step sizes. Do this by imposing a constraint on the KL divergence between the new and old policy:
        \begin{eqnarray}
        \max_{\theta} L_{\theta_{old}} (\theta)\\
        \text{subject to } D_{KL}^{s \sim \mu_{\theta_{old}}} (\theta_{old}, \theta) \leq \delta
        \end{eqnarray}
        \item This uses the average KL instead of max (the max requires the KL is bounded at all states and yields an impractical number of constraints)
    \end{itemize}
    
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{From Theory to Practice}
	
    \begin{itemize}
        \item Prior objective:
        $$ \max_{\theta} L_{\theta_{old}} (\theta)$$
        {\centering subject to $ D_{KL}^{s \sim \alert{\mu_{\theta_{old}}}} (\theta_{old}, \theta) \leq \delta$\\}
        where $ L_\pi(\Tilde{\pi}) = V(\theta) + \sum_{s} \alert{\mu_\pi (s)} \sum_{a} \Tilde{\pi}(a \mid s) \alert{A_\pi(s,a)}$ 
        \item Don't know the visitation weights nor true advantage function
        \pause
        \item Instead do the following substitutions:
        $$ \sum_{s} \mu_\pi (s) \to \frac{1}{1- \gamma} \mathbb{E}_{s\sim \mu_{\theta_{old}}} [\ldots]$$
        \item[$\leadsto$] estimate $\mu_\pi (s)$ wrt the sampled trajectories we have already
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{From Theory to Practice II}
	
    \begin{itemize}
        \item Next substitution:
        $$\sum_{a} \pi_\theta (a\mid s_n) A_{\theta_{old}} (s_n, a) \to \mathbb{E}\left[ \frac{\pi_\theta(a \mid s_n)}{q(a\mid s_n)} A_{\theta_{old}} (s_n, a) \right] $$
        \item where $q$ is some sampling distribution over actions and $s_n$ is a particular sampled state
        \item This second substitution is to use \alert{importance sampling} to estimate the desired sum, enabling the use of an alternate sampling distribution $q$ (other than the new policy $\pi_\theta$)
        \pause
        \item Third substitution:
        $$ A_{\theta_{old}} \to Q_{\theta_{old}}$$
        \item Note that the above substitutions do not change the solution to the above optimization problem
    \end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Selecting the Sampling Policy}
	
    \begin{itemize}
        \item Optimize
        $$ \max_\theta \mathbb{E}_{s\sim \mu_{\theta_{old}}, a \sim q} \left[ \frac{\pi_\theta(a \mid s_n)}{q(a\mid s_n)} A_{\theta_{old}} (s_n, a) \right]$$
        {\centering subject to $\mathbb{E}_{s\sim \mu_{\theta_{old}}} D_{KL}(\pi_{old}(\cdot \mid s), \pi_\theta (\cdot \mid s)) \leq \delta$ \\}
        \item Standard approach: sampling distribution is $q(a\mid s)$ is simply $\pi_{old}(a \mid s)$
        %\item For the vine procedure see the paper
    \end{itemize}
    
    % \centering
    % \includegraphics[width=0.5\textwidth]{images/trpo.PNG}\\
    % \lit{Schulman et al. 2015}{https://arxiv.org/abs/1502.05477}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{TRPO Algorithm~\lit{Schulman et al. 2015}{https://arxiv.org/abs/1502.05477}}

	\begin{itemize}
	    \item For $i= 1,2,\ldots$ do
	    \begin{enumerate}
	        \item Run policy for $T$ timesteps or $N$ trajectories
	        \item Estimate advantage function at all timesteps
	        \item Compute estimated policy gradient $\hat{g}$
	        \item Use CG to compute $F^{-1}g$ where F is the fisher information matrix
	        \item Do line search on surrogate loss $L$ and KL constraint
	    \end{enumerate}
	\end{itemize}



\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Common Template of Policy Gradient Algorithms}

	\begin{itemize}
	    \item For $i= 1,2,\ldots$ do
	    \begin{enumerate}
	        \item Run policy for $T$ timesteps or $N$ trajectories
	        \item At each timestep in each trajectory, compute target $Q^\pi(s_t, a_t)$ and baseline $b(s_t)$
	        \item Compute estimated policy gradient $\hat{g}$
	        \item Update the policy using $\hat{g}$, potentially constrained to a local region\\ (i.e., try to ensure monotonic improviement)
	    \end{enumerate}
	\end{itemize}



\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Gradient Summary}

\begin{itemize}
    \item Extremely popular and useful set of approaches
    \item Can input prior knowledge in the form of specifying policy parameterization
    \item You should be familiar with REINFORCE and the policy gradient template on the previous slide
    \item Understand the where different estimators can be slotted in 
    \item (You don't have to be able to derive or remember the specific formulars in TRPO for approximating the objectives and constraints)
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
