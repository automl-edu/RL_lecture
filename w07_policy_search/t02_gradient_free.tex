% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Gradient-free]{RL: Policy Search}
\subtitle{Gradient-free Optimization}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Optimization}

\begin{itemize}
	\item Policy based reinforcement learning is an \alert{optimization} problem over $\theta$
	\item[$\leadsto$] Find policy parameters $\theta^*$ that maximize $V^{\pi_{\theta^*}}(s_0)$
	\item We can use gradient-free approaches (a.k.a. black-box optimization)
	\begin{itemize}
		\item Hill climbing
		\item Simplex / amoeba / Nelder Mead
		\item Genetic algorithms
		\item Cross-Entropy method
		\item Covariance Matrix Adaptation (CMA)
	\end{itemize}
	\pause
	\smallskip
	\item gradient-free optimizers are (often) designed for
	\begin{itemize}
		\item many function evaluations $\to$ possible in RL
		\item parallel computation $\to$ possible in RL
		\item a few to hundreds of dimensions $\to$ RL?
	\end{itemize}
	\pause
	\item if we encode the policy $\pi_\theta$ as a DNN, we might have \alert{millions} of dimensions\\ (i.e., parameters in $\theta$)

\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Optimization with Evolutionary Strategies}
	
\begin{itemize}
	\item Evolutionary Strategies can perform surprisingly well nevertheless \lit{Salimans et al. 2017}{https://arxiv.org/abs/1703.03864}, \lit{Chrabaszcz et al. 2018}{https://arxiv.org/abs/1802.08842}, \lit{Fuks et al. 2019}{https://ml.informatik.uni-freiburg.de/papers/19-IJCAI_PEL.pdf}
	
\end{itemize}

\bigskip

\centering
\includegraphics[width=1.0\textwidth]{images/ES_DNN_Playing.png}
\begin{flushright}
	Sources: \lit{Chrabaszcz et al. 2018}{https://arxiv.org/abs/1802.08842}, \lit{Fuks et al. 2019}{https://ml.informatik.uni-freiburg.de/papers/19-IJCAI_PEL.pdf}
\end{flushright}
		
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c,fragile]{Policy Optimization with Evolutionary Strategies \lit{Chrabaszcz et al. 2018}{https://arxiv.org/abs/1802.08842}}
	
\vspace{-1em}
\begin{algorithm}[H]
	\scriptsize
	\DontPrintSemicolon
	\KwIn{{}\\{$\theta_0$ - Initial policy vector parameters}\\
		{$T$ - time budget}\\
		{$\lambda$ - Population size}\\
		{$\mu$ - Parent population size}\\
		{$\sigma$ - Mutation step-size}\\
		{$F(\theta)$ - Fitness function for policy evaluation}\\
	}
	\For{$j \in \{1 \ldots \mu\}$}{
		$ w_j = \frac{log(\mu+0.5)-log(j)}{\sum\nolimits_{k=1}^\mu log(\mu+0.5)-log(k)} $ \\
	}
	\For{$t=0,1, \ldots, T$}{
		\For{$i=1,2, \ldots, \lambda$}{ 
			{$\epsilon_i\sim\mathcal{N}(0,I)$}\;\\
			$s_i\leftarrow F(\theta_t+\sigma\cdot\epsilon_i)$
		}
		Sort $(\epsilon_1,\ldots,\epsilon_\lambda)$ according to $s$ in descending order\\
		{Update policy: $\theta_{t+1}\leftarrow \theta_t + \sigma\cdot\sum\nolimits_{j=1}^\mu w_j\cdot\epsilon_j$}
	}
	\KwOut{{} Return best found policy $\theta_t$}
	\caption{Canonical Evolution Strategy}\label{alg:canonical}
\end{algorithm}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c,fragile]{Progressive Episode Length \lit{Fuks et al. 2019}{https://ml.informatik.uni-freiburg.de/papers/19-IJCAI_PEL.pdf}}
	
\centering
\includegraphics[width=.70\textwidth]{images/FinalPEL.pdf}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
