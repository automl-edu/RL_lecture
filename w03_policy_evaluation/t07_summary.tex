% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Reinforcement Learning: Policy Evaluation]{Policy Evaluation}
\subtitle{Summary: Policy Evaluation}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Bias/Variance of Model-free Policy Evaluation Algorithms}

\begin{itemize}
	\item Return $G_t$ is an unbiased estimate of $V^\pi(s_t)$
	\item TD target $[r_t + \gamma V^\pi(s_{t+1})]$ is biased estimate of $V^\pi(s)$ 
	\item But often TD much lower variance than a single return $G_t$
	\begin{itemize}
		\item MC: Return function of multi-step sequence of \\ random actions, states \& rewards
		\item TD target only has one random action, reward, and next state
	\end{itemize}
	\pause
	\item MC:
	\begin{itemize}
		\item Unbiased (for first visit MC)
		\item High variance
		\item Consistent (converges to true) even with function approximation
	\end{itemize}
	\pause
	\item TD
	\begin{itemize}
		\item Some bias
		\item Lower variance
		\item TD(0) converges to true value with tabular representation
		\item TD(0) does not always converge with function approximation
	\end{itemize}
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{AB Example~\lit{Sutton \& Barto, 2018}{http://incompleteideas.net/book/RLbook2020.pdf}}
	
	\centering\vspace{-1em}
	\includegraphics[width=0.3\textwidth]{images/ab_example.png}
	
	\begin{itemize}
		\item Two states $A$, $B$ with $\gamma = 1$
		\item Given $8$ episodes of experience:
		\begin{itemize}
			\item $A, 0, B, 0$
			\item $B, 1$ (observed $6$ times)
			\item $B, 0$ 
		\end{itemize}
		\item Under batch (offline) solution for this finite set of observations, what do MC and TD(0) converge to?
		\item Imagine running TD updates over data an infinite number of times?
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{AB Example~\lit{Sutton \& Barto, 2018}{http://incompleteideas.net/book/RLbook2020.pdf}}

\begin{itemize}
	\item Given $8$ episodes of experience:
	\begin{itemize}
		\item $A, 0, B, 0$
		\item $B, 1$ (observed $6$ times)
		\item $B, 0$ 
	\end{itemize}
	\item For $B$:
	\begin{itemize}
		\item MC: $V(B) = \frac{6}{8} =  0.75$
		\item TD: $V(B) = \frac{6}{8} =  0.75$
	\end{itemize} 
	\pause
	\item For $A$:
	\begin{itemize}
		\item MC: only one episode with $A$ $\leadsto$ $V(A) = 0$ 
		\item TD: bootstraps from $V(B)$ $\leadsto$ $V(A) = 0.75$
$$ V^\pi(s) = V^\pi(s) + \alpha (\underbrace{[r_t + \gamma V^\pi (s_{t+1})]}_{\text{TD target}} - V^\pi(s))$$
	\end{itemize}	
	\medskip
	\item[$\leadsto$] Monte Carlo in batch setting converges to minimal \alert{MSE} (mean squared error)
	\item[$\leadsto$] TD(0) converges to DP policy $V^\pi$for the MDP with the \alert{maximum likelihood model estimates}
	
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Efficiency}
	
	\begin{itemize}
		\item Data efficiency \& Computational efficiency
		\item In simplest TD, use $(s,a,r,s')$ once to update $V(s)$
		\begin{itemize}
			\item $O(1)$ operation per update
			\item In an episode of length $L$, $O(L)$
		\end{itemize}		
		\item In MC have to wait till episode finishes, then also $O(L)$
		\item MC can be more data efficient than simple TD in non-Markov domains
		\item TD can exploit Markov structure $\leadsto$ leveraging this is helpful
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Summary: Policy Evaluation}
	
	Estimating the expected return of a particular policy if don’t have access to true MDP models. Example: evaluating average purchases per session of new product recommendation system
	
	\begin{itemize}
		\item Dynamic Programming
		\item Monte Carlo policy evaluation
		\begin{itemize}
			\item Policy evaluation when we don’t have a model of how the world works
		\end{itemize}
		\item Temporal Difference (TD)
		\item Metrics to evaluate and compare algorithms
		\begin{itemize}
			\item Robustness to Markov assumption
			\item Bias/variance characteristics
			\item Data efficiency
			\item Computational efficiency
		\end{itemize}
	\end{itemize}

	
\end{frame}
%-----------------------------------------------------------------------



%-----------------------------------------------------------------------
\end{document}
