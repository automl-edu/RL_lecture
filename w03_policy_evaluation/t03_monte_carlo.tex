% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Reinforcement Learning: Policy Evaluation]{Policy Evaluation}
\subtitle{Monte Carlo Evaluation}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Evaluation without a model}

\begin{itemize}
	\item Goal: Policy Evaluation without a model
	\begin{itemize}
		\item  	Given data and/or ability to interact with the environment
		\item Efficiently compute a good estimate of a policy $\pi$
	\end{itemize}
	\smallskip
	\item For example Estimate expected total purchases during an online shopping session for a new automated product recommendation policy
\end{itemize}



\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Monte Carlo (MC) Policy Evaluation}

\begin{itemize}
	\item $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots$ in MDP $M$ under policy $\pi$
	\smallskip
	\item $V^\pi(s) \approx \mathbb{E}_{T \sim \pi} [G_t \mid s_t = s]$
	\begin{itemize}
		\item Expectation over trajectories $T$ generated by following $\pi$
	\end{itemize}
 	\smallskip
	\item Simple idea: Value = mean return
	\item If trajectories are all finite, sample a set of trajectories \& average returns
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Monte Carlo (MC) Policy Evaluation}

\begin{itemize}
	\item If trajectories are all finite, sample a set of trajectories \& average returns
	\item Does \alert{not} require MDP dynamics/rewards
	\item No bootstrapping
	\item Does not assume the state is Markov
	\item Can only be applied to episodic MDPs
	\begin{itemize}
		\item Averaging over returns from a complete episode
		\item Requires each episode to terminate
	\end{itemize}
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Monte Carlo (MC) Policy Evaluation}

\begin{itemize}
	\item Aim: estimate $V^\pi (s)$ given episodes generated under policy $\pi$
	\begin{itemize}
		\item $s_1, a_1, r_1, s_2, a_2, r_2, \ldots$ where the actions are sampled from $\pi$
	\end{itemize}
	\item $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots$ in MDP $M$ under policy $\pi$
	\item $V^\pi (s) = \mathbb{E} [G_t \mid s_t = s]$
	\item MC computes the empirical mean return
	\item Often do this in an incremental fashion
	\begin{itemize}
		\item After each episode, update the estimate of $V^\pi$
	\end{itemize}
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{First-Visit Monte Carlo (MC) On Policy Evaluation}

Initialize $N(s) = 0$, $G(s) = 0$ $\forall s \in S$\\
Loop 
\begin{itemize}
	\item Sample episode $i = s_{i,1}, a_{i,1}, r_{i,1}, s_{i,2}, a_{i,2}, r_{i,2}, \ldots,s_{i,n}$
	\item Define $G_{i,t} = r_{i,t} + \gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + \ldots$
	\item For each state $s$ visited in episode $i$
	\begin{itemize}
		\item for first time $t$ that state $s$ is visited in episode $i$
		\begin{itemize}
			\item Increment counter of total first visits: $N(s) = N(s) + 1$
			\item Increment total return $G(s) = G(s) + G_{i,t}$
			\item Update estimate $V^\pi (s) = G(s) /N(s)$
		\end{itemize}
	\end{itemize}
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
