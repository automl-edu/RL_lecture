% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Reinforcement Learning: Policy Evaluation]{Policy Evaluation}
\subtitle{Monte Carlo Evaluation: Bias and Variance for MC}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{First-Visit Monte Carlo (MC) On Policy Evaluation}

\begin{algorithm}[H]
    \caption{First Visit Monte Carlo}
    \DontPrintSemicolon
    \LinesNotNumbered
    
    \KwIn{{}\\ $\pi$ - Policy to evaluate}
    
    \textbf{Initialise}: $N(s) \gets 0 \quad \forall s \in S$ \tcp*{Amount of times $s$ visited}
    \textbf{Initialise}: $G(s) \gets 0 \quad \forall s \in S$ \tcp*{Accumulated discounted returns}
    \While{not converged}{
        Sample episode $i = s_{i,1}, a_{i,1}, r_{i,1}, s_{i,2}, a_{i,2}, r_{i,2}, \ldots,s_{i,n}$
        
        \If{s visited for the first time in current episode}{
            $N(s) \leftarrow N(s) + 1$
            
            $G(s) \leftarrow G(s) + G_{i,t}$
            
            $V^\pi (s) \leftarrow G(s)/N(s)$
        }
    }
    \KwOut{{} Return $V^\pi$}
\end{algorithm}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recap: Bias, Variance and MSE}

\begin{itemize}
	\item Consider a statistical model parameterized by $\theta$ where $\theta$ determines
	a probability distribution over observed data $P(x|\theta)$
	\item Consider a statistic $\hat{\theta}$ that provides an estimate of $\theta$ and is a function of
	observed data $x$
	\begin{itemize}
		\item E.g. for a Gaussian distribution with known variance, the average of a set of
		i.i.d data points is an estimate of the mean of the Gaussian
	\end{itemize}
	\item Definition: the bias of an estimator $\hat{\theta}$ is:
	$$ Bias_{\theta} (\hat{\theta}) = \mathbb{E}_{x\mid \theta} [\hat{\theta}] - \theta$$
	\item Definition: the variance of an estimator $\hat{\theta}$ is:
	$$ Var (\hat{\theta}) = \mathbb{E}_{x\mid \theta} [(\hat{\theta} - \mathbb{E}[\hat{\theta}])^2] $$
	\item Definition: mean squared error (MSE) of an estimator $\hat{\theta}$ is
	$$MSE(\hat{\theta}) = Var(\hat{\theta})  + Bias_{\theta} (\hat{\theta}) $$
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{First-Visit Monte Carlo (MC) On Policy Evaluation}

\begin{algorithm}[H]
    \caption{First Visit Monte Carlo}
    \DontPrintSemicolon
    \LinesNotNumbered
    
    \KwIn{{}\\ $\pi$ - Policy to evaluate}
    
    \textbf{Initialise}: $N(s) \gets 0 \quad \forall s \in S$ \tcp*{Amount of times $s$ visited}
    \textbf{Initialise}: $G(s) \gets 0 \quad \forall s \in S$ \tcp*{Accumulated discounted returns}
    \While{not converged}{
        Sample episode $i = s_{i,1}, a_{i,1}, r_{i,1}, s_{i,2}, a_{i,2}, r_{i,2}, \ldots,s_{i,n}$
        
        \If{s visited for the first time in current episode}{
            $N(s) \leftarrow N(s) + 1$
            
            $G(s) \leftarrow G(s) + G_{i,t}$
            
            $V^\pi (s) \leftarrow G(s)/N(s)$
        }
    }
    \KwOut{{} Return $V^\pi$}
\end{algorithm}
\end{frame}

%-----------------------------------------------------------------------
%----------------------------------------------------------------------


\begin{frame}[c]{Properties and Limitations of First-Visit Monte Carlo (MC)}

Properties:
\begin{itemize}
	\item $V^\pi$ estimator is an unbiased estimator of true $\mathbb{E}_\pi [G_t \mid s_t = s]$
	\item By law of large numbers, as $N(s) \to \inf, V^\pi(s) \to \mathbb{E}_\pi [G_t \mid s_t = s]$
	\item \alert{every}-visit MC estimator:
	\begin{itemize}
		\item is biased estimator of $V^\pi$ (observations are correlated $\leadsto$ not i.i.d)
		\item often better RMSE, because more data per state
	\end{itemize}

\end{itemize}

Limitations:
\begin{itemize}
		\item Generally high variance estimator
		\begin{itemize}
			\item Reducing variance can require a lot of data
			\item In cases where data is very hard or expensive to acquire, or the stakes are
			high, MC may be impractical
		\end{itemize}
		\medskip
		\pause
		\item Requires episodic settings
		\begin{itemize}
			\item Episode must end before data from the episode can be used to update $V$

		\end{itemize}
		
	\end{itemize}
\end{frame}

%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Monte Carlo (MC) Policy Evaluation Summary}
	
	\begin{itemize}
		\item Aim: estimate $V^\pi(s)$ given episodes generated under policy $\pi$
		\begin{itemize}
			\item $s_{i,1}, a_{i,1}, r_{i,1}, s_{i,2}, a_{i,2}, r_{i,2}, \ldots$ where the actions are sampled from $\pi$
			\item $G_{i,t} = r_{i,t} + \gamma r_{i,t+1} + \gamma^2 r_{i,t+2} + \ldots$ under policy $\pi$
			\item $V^\pi (s) = \mathbb{E}[G_t \mid s_t = s]$
		\end{itemize}
		\item Simple: Estimates expectation by empirical average (given episodes sampled from the policy of interest)
		\item Updates $V$ estimate using sample of return to approximate the expectation
		\item No bootstrapping
		\item Does not assume Markov process
		%\item Converges to true value under some (generally mild) assumptions
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------

%-----------------------------------------------------------------------
\end{document}
