% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[Reinforcement Learning: Policy Evaluation]{Policy Evaluation}
\subtitle{Dynamic Programming}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dynamic Programming for Evaluating Value of Policy $\pi$}

\begin{itemize}
	\item Initialize $V^\pi_0(s) = 0$ for all $s\in S$
	\item For $k=1$ until convergence
	\begin{itemize}
		\item For all $s$ in $S$
		$$V^\pi_k (s)  = r(s, \pi(s)) + \gamma \sum_{s' \in S} p(s' \mid s, \pi(s)) V^\pi_{k-1} (s') $$
	\end{itemize}
	\item $V^\pi_k (s)$ is exact value of $k$-horizon value of state $s$ under policy $\pi$
	\item $V^\pi_k (s)$ is an estimate of infinite horizon value of state $s$ under policy $\pi$
	
	$$V^\pi (s) = \mathbb{E}_\pi [G_t \mid s_t = s] \approx \mathbb{E} [r_t + \gamma V_{k-1} \mid s_t = s]$$
\end{itemize}



\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dynamic Programming for Evaluating Value of Policy $\pi$}

$$V^\pi (s) = \mathbb{E}_\pi [G_t \mid s_t = s] \approx \mathbb{E} [r_t + \gamma V_{k-1} \mid s_t = s]$$

\begin{center}
	\includegraphics[width=0.25\textwidth]{images/state_action_graph.png}
\end{center}



\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Dynamic Programming for Evaluating Value of Policy $\pi$}

$$V^\pi (s) = \mathbb{E}_\pi [G_t \mid s_t = s] \approx \mathbb{E} [r_t + \gamma V_{k-1} \mid s_t = s]$$

\begin{center}
	\includegraphics[width=0.25\textwidth]{images/state_action_graph2.png}
\end{center}

\vspace{-1cm}
\pause
\begin{itemize}
	\item[$\leadsto$] DP computes one step update at a time
	\item[$\leadsto$] bootstrapping the rest of the expected return\newline by the value estimate $V_{k-1}$
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{What do we mean by bootstrapping?}
\begin{itemize}
    \item We want to get an exact state value $V^\pi(s)$. 
    \item To do this, we would ideally need to roll the whole sequence out
    \item \textbf{Problem:} What if the sequence is too long, or even infinite? 
    \item \textbf{Idea:} We treat the current state value as an estimate of the exact value and run with it
    \begin{itemize}
        \item This adds bias to the process since we are basically treating the estimation as truth
    \end{itemize}
    \item \textbf{Key:} If we refine this estimation over time, then we can get closer to the actual value 
    \item \alert{"Pull ourselves out, by the bootstraps"    }
\end{itemize}
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Evaluation: $V^\pi = \mathbb{E}[G_t \mid s_t = s]$}

\begin{itemize}
	\item $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \ldots$ in MDP $M$ under policy $\pi$
	\item Dynamic Programming
	\begin{itemize}
		\item $V^\pi (s)  = \mathbb{E}_\pi [r_t + \gamma V_{k-1} \mid s_t = s]$
		\item \alert{Requires model of MDP $M$} TODO UPDATE SLIDE!
		\item Bootstraps future return using value estimate
		\item Requires Markov assumption: bootstrapping regardless of history
	\end{itemize}
	\pause
	\medskip
	\item What if we don't know the dynamic model P and/or reward model R? 
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
