% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Function Approximation]{Function Approximation}
\subtitle{VFA: Temporal Difference}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recall: Temporal Difference Learning w/ Lookup Table}
	
	\begin{itemize}
		\item Uses bootstrapping and sampling to approximate $V^\pi$
		\item Updates $V^\pi(s)$ after each transition $(s,a,r,s')$
		$$V^\pi(s) = V^\pi (s) + \alpha(r + \gamma V^\pi (s') - V^\pi(s)) $$
		\item Target is $r + \gamma V^\pi(s')$, a biased estimate of the true value $V^\pi(s)$
		\item Represent value for each state with a separate \alert{table entry}
	\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Temporal Difference (TD(0)) Learning with Value
		Function Approximation}
	
	\begin{itemize}
		\item Uses bootstrapping and sampling to approximate $V^\pi$
		\item Updates $V^\pi(s)$ after each transition $(s,a,r,s')$
		$$V^\pi(s) = V^\pi (s) + \alpha(r + \gamma V^\pi (s') - V^\pi(s)) $$
		\item Target is $r + \gamma V^\pi(s')$, a biased estimate of the true value $V^\pi(s)$
		\item In value \alert{function approximation}, target is $r + \gamma \hat{V}^\pi (s'; \vect{w})$, a biased and approximated estimate of the true value $V^\pi(s)$
		\item 3 forms of approximation: 
		\begin{itemize}
			\item sampling
			\item bootstrapping
			\item VFA
		\end{itemize}
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Temporal Difference (TD(0)) Learning with Value
		Function Approximation}
	
	\begin{itemize}
		\item In value function approximation, target is $r + \gamma \hat{V}^\pi (s'; \vect{w})$, a biased and approximated estimate of the true value $V^\pi(s)$
		\item Can reduce doing TD(0) learning with value function approximation to supervised learning on a set of data pairs
		$$\langle  s_1, r_1+ \gamma \hat{V}^\pi (s_2; \vect{w}) \rangle, \langle  s_2, r_2+ \gamma \hat{V}^\pi (s_3; \vect{w}) \rangle, \ldots $$
		\item Find weights to minimize mean squared error
		\begin{eqnarray*}
		J(\vect{w}) &=& \mathbb{E}_\pi [(V(s_{j+1}, \vect{w}) - \hat{V}^\pi(s_j;\vect{w}))^2]\\
		&\approx& \mathbb{E}_\pi [(\underbrace{r_j + \gamma \hat{V}^\pi(s_{j+1}, \vect{w})}_{\text{target approximation}} - \hat{V}^\pi(s_j;\vect{w}))^2]
		\end{eqnarray*}

	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Temporal Difference (TD(0)) Learning with Value
		Function Approximation}
	
	\begin{itemize}
		\item In value function approximation, target is $r + \gamma \hat{V}^\pi (s'; \vect{w})$, a biased and approximated estimate of the true value $V^\pi(s)$
		\item Can reduce doing TD(0) learning with value function approximation to supervised learning on a set of data pairs
		$$\langle  s_1, r_1+ \gamma \hat{V}^\pi (s_2; \vect{w}) \rangle, \langle  s_2, r_2+ \gamma \hat{V}^\pi (s_3; \vect{w}) \rangle, \ldots $$
		\item In linear TD(0):
	\end{itemize}
\vspace{-1em}
\begin{eqnarray}
\Delta \vect{w} &=& \alpha( r + \gamma \hat{V}^\pi(s'; \vect{w}) - \hat{V}^\pi(s;\vect{w})) \nabla_{\vect{w}} \hat{V}^\pi (s;\vect{w})\nonumber\\
&=& \alpha( r + \gamma \hat{V}^\pi(s'; \vect{w}) - \hat{V}^\pi(s;\vect{w})) \vect{x}(s)\nonumber\\
&=& \alpha( r + \gamma \vect{x}(s')^T \vect{w} - \vect{x}(s)^T\vect{w}) \vect{x}(s)\nonumber
\end{eqnarray}

\alert{Remark:} $r_j + \gamma \hat{V}^\pi(s_{j+1}, \vect{w})$ is the target; the target won't be updated.\\  Therefore, $\hat{V}^\pi(s'; \vect{w})$ is treated as a constant in the derivative.
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Temporal Difference (TD(0)) Learning with Value
		Function Approximation}

Initialize $\vect{w}= 0$, $k=1$;\\
Loop
	\begin{itemize}
		\item Sample tuple $(s_k, a_k, r_k, s_{k+1})$ given $\pi$
		\item Update weights:
		$$ \vect{w} = \vect{w} + \alpha( r + \gamma \vect{x}(s')^T \vect{w} - \vect{x}(s)^T\vect{w}) \vect{x}(s) $$
		\item $k = k + 1$
	\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Convergence Guarantees for LVF
		Approximation for Policy Evaluation}
	
\begin{itemize}
	\item Define the mean squared error of a linear value function
	approximation for a particular policy $\pi$ relative to the true value as
	$$\text{MSVE}(\vect{w}) = \sum_{s\in S} d(s) (V^\pi(s) - \hat{V}^\pi (s;\vect{w}))^2$$
	\item where
	\begin{itemize}
		\item $d(s):$ stationary distribution of $\pi$ in the true decision process
		\item $\hat{V}(s;\vect{w}) = \vect{x}(s)^T \vect{w}$, a linear value function approximation
	\end{itemize}
	\item TD(0) policy evaluation with VFA converges to weights $\vect{w}_{TD}$ which is a constant factor of the minimum mean squared error possible:
	$$\text{MSVE}(\vect{w}_{TD}) \leq \frac{1}{1-\gamma} \min_\vect{w}\sum_{s\in S} d(s) (V^\pi(s) - \hat{V}(s;\vect{w}))^2$$
\end{itemize}

	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
