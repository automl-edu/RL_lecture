% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Function Approximation]{Function Approximation}
\subtitle{Gradient Descent and Linear Models}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Overview}
	
	
\begin{itemize}
	\item Represent a (state-action/state) value function with a parameterized
	function instead of a table
\end{itemize}

\begin{center}
	\includegraphics[width=0.6\textwidth]{images/vfa.png}
\end{center}

\begin{itemize}
	\item \alert{Which function approximator}
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Function Approximators}
	
	
	\begin{itemize}
		\item Many possible function approximators including
		\begin{itemize}
			\item linear combinations of features
			\item Neural networks
			\item Decision trees
			\item Nearest neighbors 
			\item Fourier / wavelet bases
		\end{itemize}
		\item Focus on differentiable function approximators
		\item Let's start with linear feature representations
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recap: Gradient Descent}
	
	
	\begin{itemize}
		\item Consider a function $J(\vect{w})$ that is differentiable function of a parameter vector $\vect{w}$
		\item Goal is to find parameter $\vect{w}$ that minimizes $J$
		\item The gradient of $J(\vect{w})$ is
	\end{itemize}
	$$
	\nabla J(\vect{w}) = \left[ \frac{\partial J}{\vect{w}_1} \ldots \frac{\partial J}{\vect{w}_n} \right]
	$$
	$$\vect{w}_t = \vect{w}_{t-1} - \alpha \nabla_{\vect{w}} J(\vect{w}_{t-1})$$
	
	where $\alpha$ is the learning rate.

	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Value Function Approximation for Policy Evaluation with
		an Oracle}
	
	
	\begin{itemize}
		\item First assume we could query any state $s$ and an \alert{oracle} would return
		the true value for $V^\pi (s)$
		\item The objective was to find the best approximate representation of $V^\pi$
		given a particular parameterized function
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Stochastic Gradient Descent}
	
	
	\begin{itemize}
		\item Goal: Find the parameter vector $\vect{w}$ that minimizes the loss between a
		true value function $V^{\pi}(s)$ and its approximation $\hat{V}^\pi(s; \vect{w})$ as
		represented with a particular function class parameterized by $\vect{w}$.
		\item Generally use mean squared error and define the loss as 
		$$ J(\vect{w}) = \mathbb{E}_{\pi} [(V^\pi(s) - \hat{V}^\pi(s;\vect{w}))^2]$$
		\item Use gradient descent to find a local minimum 
		$$ \Delta \vect{w} = - \frac{1}{2} \alpha \nabla_{\vect{w}} J(\vect{w})$$
		\item Stochastic gradient descent (SGD) uses a finite number of samples to compute an approximate gradient:
		$$ \nabla_{\vect{w}} J(\vect{w}) = \nabla_{\vect{w}} \mathbb{E}_{\pi}[(V^\pi (s) - \hat{V}^\pi (s; \vect{w}))^2]$$
		$$= \mathbb{E}_\pi [-2 (V^\pi(s) - \hat{V}^\pi (s;\vect{w})) \nabla_{\vect{w}} \hat{V}(s;\vect{w})]$$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Model Free VFA Policy Evaluation}
	
	
	\begin{itemize}
		\item In practice, we donâ€™t actually have access to an oracle to tell true $V^\pi(s)$ for any
		state $s$
		\item Now consider how to do model-free value function approximation for
		prediction / evaluation / policy evaluation without a model
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Model Free VFA Prediction / Policy Evaluation}
	
	
	\begin{itemize}
		\item Recall model-free policy evaluation
		\begin{itemize}
			\item Following a fixed policy $\pi$ (or had access to prior data)
			\item Goal is to estimate $V^\pi$ and/or $Q^\pi$
		\end{itemize}
		\item Maintained a lookup table to store estimates $V^\pi$ and/or $Q^\pi$
		\item Updated these estimates after each episode (Monte Carlo methods)
		or after each step (TD methods)
		\item New: in value function approximation, change the estimate
		update step to include fitting the function approximator
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Model Free VFA Prediction / Policy Evaluation}
	
	
	\begin{itemize}
		\item Use a feature vector to represent a state $s$
	\end{itemize}

$$\vect{x}(s) = \begin{pmatrix}
\vect{x}_1(s)\\
\vect{x}_2(s)\\
\ldots\\
\vect{x}_n(s)
\end{pmatrix} $$

\begin{itemize}
	\item For table lookups, we have not really needed that because we only needed to know which table entry to look up
\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Linear Value Function Approximation for Prediction With
		An Oracle}
	
	
	\begin{itemize}
		\item Represent a value function (or state-action value function) for a
		particular policy with a weighted linear combination of features
		$$ \hat{V}^\pi(s; \vect{w}) = \sum_{j=1}^n \vect{x}_j (s) \vect{w}_j = \vect{x}(s)^T\vect{w}$$
		\item Objective function is 
		$$ J(\vect{w}) = \mathbb{E}[(V^\pi(s) - \hat{V}^\pi(s; \vect{w}))^2]$$
		\item Recall weight update:
		$$ \Delta \vect{w} = - \frac{1}{2} \alpha \nabla_{\vect{w}} J (\vect{w})$$
		\item Update (- step size $\times$ prediction error $\times$ feature value)
		$$ \Delta \vect{w} = -\frac{1}{2} \alpha(-2(V^\pi(s) - \vect{x}(s)^T \vect{w})) \vect{x}(s)$$
		
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
