% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Function Approximation]{Function Approximation}
\subtitle{VFA: Monte Carlo}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Overview}
	
	
\begin{itemize}
	\item Represent a (state-action/state) value function with a parameterized
	function instead of a table
\end{itemize}

\begin{center}
	\includegraphics[width=0.6\textwidth]{images/vfa.png}
\end{center}


\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Monte Carlo Value Function Approximation (VFA)}
	\begin{itemize}
		\item Return $G_t$ is an unbiased but noisy sample of the true expected return $V^\pi(s_t)$
		\item Therefore, we can reduce MC VFA to doing supervised learning on a set of (state, return) pairs; $\langle s_1, G_1 \rangle, \langle s_2, G_2 \rangle,\ldots, \langle s_T, G_T \rangle$
		\begin{itemize}
			\item Substitute $G_t$ for the true $V^\pi(s)$ when fit function approximator
		\end{itemize}
		\item Concretely when using linear VFA for policy evaluation
		\vspace{-.2cm}
		\begin{eqnarray*}
		 \Delta \vect{w} &=&- (1/2) \alpha \nabla_{\vect{w}} J (\vect{w})\\
          &=& - (1/2) \alpha \nabla_{\vect{w}} (-2(G_t - \vect{x}(s)^T \vect{w})) \hat{V}(s_t; \vect{w})\\
          &=& \alpha (G_t - \hat{V} (s_t; \vect{w})) \nabla_{\vect{w}}\hat{V}(s_t; \vect{w}) \\
		 &=& \alpha (G_t - \hat{V} (s_t; \vect{w})) \vect{x}(s_t)\\
		 &=& \alpha (G_t - \vect{x}(s_t)^T \vect{w}) \vect{x}(s_t)
		\end{eqnarray*}
		
		\item Note: $G_t$ may be a very noisy estimate of true return
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MC Linear Value Function Approximation for Policy
		Evaluation}

    \begin{algorithm}[H]
        \caption{Monte Carlo Value Function Approximation}
        \DontPrintSemicolon
        \LinesNotNumbered
        \KwIn{{}\\
        $\alpha$ - Learning rate,\\
        $K$ - number of episodes,\\
        $L_k$ - episode Length}
        \textbf{Initialise}:  $\vect{w}= \mathbf{0}$, $k=1$\\
        \While{$k \leq K$}{
            Sample $k-th$ episode $s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, a_{k,2}, r_{k,2}, \ldots$\\
            \For{$t \gets 1,2,...L_k$}{
                \If{First visit to state}{
                    {$G_t(s) \gets \sum_{j}^{L_k} r_{k,j}$}\\
                    Update weights by $\alpha (G_t - \vect{x}(s_{k,t})^T \vect{w}) \vect{x}(s_{k,t})$
                }
            }
        }
    \KwOut{Return: $\hat{V}(\cdot)$}
    \end{algorithm}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Convergence Guarantees for LVF Approx. for Policy Evaluation}
	
	\begin{itemize}
		\item For infinite horizon, the Markov Chain defined by an MDP with a particular policy will eventually converge to a probability distribution over states $d(s)$
		\item $d(s)$ is called the stationary distribution over states of $\pi$
		\item $\sum_{s} d(s) = 1$
		\item $d(s)$ satisfies the following balance equation:
		$$ d(s') = \sum_{s} \sum_{a} \pi(a \mid s) p(s' \mid s,a) d(s) $$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Convergence Guarantees for LVF
		Approx. for Policy Evaluation}

	\lit{Tsitsiklis and Van Roy. 1997}{https://ieeexplore.ieee.org/document/580874}
	\begin{itemize}
		\item Define the mean squared error of a linear value function approximation for a particular policy $\pi$  relative to the true value as 
		$$\text{MSVE}(\vect{w}) = \sum_{s \in S} d(s) (V^\pi (s) - \hat{V}^\pi(s;\vect{w}))^2 $$
		\item where
		\begin{itemize}
			\item $d(s)$: stationary distribution of $\pi$ in the true decision process
			\item $\hat{V}^\pi(s;\vect{w}) = \vect{x}(s)^T\vect{w}$, a linear value function approximation
		\end{itemize}
		\item Monte Carlo policy evaluation with VFA converges to the weights $\vect{w}_{MC}$ which has the minimum mean squared error possible:
		$$\text{MSVE}(\vect{w}_{MC}) = \min_{\vect{w}}\sum_{s \in S} d(s) (V^\pi (s) - \hat{V}^\pi(s;\vect{w}))^2 $$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
