% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Function Approximation]{Function Approximation}
\subtitle{VFA: Monte Carlo}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Overview}
	
	
\begin{itemize}
	\item Represent a (state-action/state) value function with a parameterized
	function instead of a table
\end{itemize}

\begin{center}
	\includegraphics[width=0.6\textwidth]{images/vfa.png}
\end{center}

\begin{itemize}
	\item \alert{Which function approximator}
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Monte Carlo Value Function Approximation (VFA)}
	
	
	\begin{itemize}
		\item Return $G_t$ is an unbiased but noisy sample of the true expected return $V^\pi(s_t)$
		\item Therefore, we can reduce MC VFA to doing supervised learning on a set of (state, return) pairs; $\langle s_1, G_1 \rangle, \langle s_2, G_2 \rangle,\ldots, \langle s_T, G_T \rangle$
		\begin{itemize}
			\item Substitute $G_t$ for the true $V^\pi(s)$ when fit function approximator
		\end{itemize}
		\item Concretely when using linear VFA for policy evaluation
		
		\begin{eqnarray}
		 \Delta \vect{w} &=& \alpha (G_t - \hat{V} (s_t; \vect{w})) \nabla_\vect{w}\hat{V}(s_t; \vect{w}) \nonumber\\
		 &=& \alpha (G_t - \hat{V} (s_t; \vect{w})) \vect{x}(s_t) \nonumber\\
		 &=& \alpha (G_t - \vect{x}(s_t)^T \vect{w}) \vect{x}(s_t) \nonumber
		\end{eqnarray}
		
		\item Note: $G_t$ may be a very noisy estimate of true return
		\item Note(2): We dropped the factor $2$ and see it as part of $\alpha$
		
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{MC Linear Value Function Approximation for Policy
		Evaluation}
	

Initialize $\vect{w}= \mathbf{0}$, $k=1$\\
Loop	
	\begin{itemize}
		\item Sample $k$-th episode $s_{k,1}, a_{k,1}, r_{k,1}, s_{k,2}, a_{k,2}, r_{k,2}, \ldots$
		\item for $t=1, \ldots, L_k$ do
		\begin{itemize}
			\item If First visit to $s_{k,t}$ in episode $k$ then
			\begin{itemize}
					\item $G_t(s) = \sum_{j}^{L_k} r_{k,j}$
					\item Update weights by $\alpha (G_t - \vect{x}(s_{k,t})^T \vect{w}) \vect{x}(s_{k,t})$
			\end{itemize}
		\end{itemize}
	\item $k = k + 1$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Convergence Guarantees for LVF Approx. for Policy Evaluation}
	
	\begin{itemize}
		\item For infinite horizon, the Markov Chain defined by an MDP with a particular policy will eventually converge to a probability distribution over states $d(s)$
		\item $d(s)$ is called the stationary distribution over states of $\pi$
		\item $\sum_{s} d(s) = 1$
		\item $d(s)$ satisfies the following balance equation:
		$$ d(s') = \sum_{s} \sum_{a} \pi(a \mid s) p(s' \mid s,a) d(s) $$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Convergence Guarantees for LVF
		Approx. for Policy Evaluation}
		
	\lit{Tsitsiklis and Van Roy. 1997}{https://ieeexplore.ieee.org/document/580874}
	\begin{itemize}
		\item Define the mean squared error of a linear value function approximation for a particular policy $\pi$  relative to the true value as 
		$$\text{MSVE}(\vect{w}) = \sum_{s \in S} d(s) (V^\pi (s) - \hat{V}^\pi(s;\vect{w}))^2 $$
		\item where
		\begin{itemize}
			\item $d(s)$: stationary distribution of $\pi$ in the true decision process
			\item $\hat{V}^\pi(s;\vect{w}) = \vect{x}(s)^T\vect{w}$, a linear value function approximation
		\end{itemize}
		\item Monte Carlo policy evaluation with VFA converges to the weights $\vect{w}_{MC}$ which has the minimum mean squared error possible:
		$$\text{MSVE}(\vect{w}_{MC}) = \min_{\vect{w}}\sum_{s \in S} d(s) (V^\pi (s) - \hat{V}^\pi(s;\vect{w}))^2 $$
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
