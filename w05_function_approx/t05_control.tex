% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Function Approximation]{Function Approximation}
\subtitle{Control using VFA}


\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Control using Value Function Approximation}
	
	\begin{itemize}
		\item Use value function approximation to represent state-action values $\hat{Q}^\pi(s,a;\vec{w}) \approx Q^\pi$
		\item Interleave
		\begin{itemize}
			\item Approximate policy evaluation using value function approximation
			\item Perform $\epsilon$-greedy policy improvement
		\end{itemize}
		\item Can be unstable. Generally involves intersection of the following:
		\begin{itemize}
			\item Function approximation
			\item Bootstrapping
			\item \alert{Off-policy learning}
		\end{itemize}
	\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Action-Value Function Approximation with an Oracle}

	\begin{itemize}
		\item $\hat{Q}^\pi(s,a;\vec{w}) \approx Q^\pi$
		\item Minimize the mean-squared error between the true action-value function $Q^\pi(s,a)$ and the approximate action-value function:
		$$J(\vec{w}) = \mathbb{E}_\pi [(Q^\pi(s,a) - \hat{Q}^\pi(s,a;\vec{w}))^2] $$
		\item Use stochastic gradient descent to find a local minimum
		\begin{align*}
			-\frac{1}{2}\nabla_{\vec{w}} J(\vec{w}) &= \mathbb{E}\left[ (Q^\pi(s,a) - \hat{Q}^\pi(s,a;\vec{w})) \nabla_{\vec{w}} \hat{Q}^\pi(s,a;\vec{w}) \right] \\
			\Delta \vec{w} &= -\frac{1}{2}\alpha\nabla_{\vec{w}} J(\vec{w})
		\end{align*}
		\item Stochastic gradient descent (SGD) samples the gradient
	\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Linear State Action Value Function Approximation with an Oracle}

	\begin{itemize}
		\item Use features to represent both the state and action
		$$\vec{x}(s,a) = \begin{pmatrix}
			\vec{x}_1(s,a)\\
			\vec{x}_2(s,a)\\
			\ldots\\
			\vec{x}_n(s,a)
		\end{pmatrix} $$
		\item Represent state-action value function with a weighted linear
		combination of features
		$$\hat{Q}(s,a;\vec{w}) = \vec{x}(s,a)^T \vec{w} = \sum_{j=1}^n x_j(s,a)w_j $$
		\item Stochastic gradient descent update
		$$\nabla_{\vec{w}} J(\vec{w}) = \nabla_{\vec{w}} \mathbb{E}_\pi [(Q^\pi(s,a) - \hat{Q}^\pi(s,a;\vec{w}))^2] $$
	\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Incremental Model-Free Control Approaches}

	\begin{itemize}
		\item Similar to policy evaluation, true state-action value function for a state is unknown and so substitute a target value
		\item In Monte Carlo methods, use a return $G_t$ as a substitute target
		$$\Delta \vec{w} = \alpha(G_t - \hat{Q}(s_t,a_t; \vec{w})) \nabla_{\vec{w}} \hat{Q}(s_t, a_t; \vec{w}) $$
		\item For SARSA instead use a TD target $r+ \gamma \hat{Q}(s', a'; \vec{w})$ which leverages the current function approximations value
		$$\Delta \vec{w} = \alpha (r + \gamma \hat{Q}(s',a';\vec{w}) - \hat{Q}(s,a;\vec{w})) \nabla_{\vec{w}}\hat{Q}(s,a;\vec{w}) $$
		\item For Q-learning instead use a TD target $r + \gamma \max_{a'} \hat{Q}(s',a';\vec{w})$ which leverages the max of the current function approximations value
		$$\Delta \vec{w} = \alpha (r + \gamma \max_{a'} \hat{Q}(s',a';\vec{w}) - \hat{Q}(s,a;\vec{w})) \nabla_{\vec{w}}\hat{Q}(s,a;\vec{w}) $$
	\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
