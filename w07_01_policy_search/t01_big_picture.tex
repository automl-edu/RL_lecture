% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Big Picture]{RL: Policy Search}
\subtitle{The Big Picture}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy-Based Reinforcement Learning}

\begin{itemize}
	\item In the last lecture, we approximated the value or action-value function
	using parameters $\vect{w}$,
	$$V_{\vect{w}}(s) \approx V^\pi(s)$$
	$$Q_{\vect{w}}(s,a) \approx Q^\pi (s,a) $$
	\item A policy was generated directly from the value function
	\begin{itemize}
		\item e.g., using $\epsilon$-greedy
	\end{itemize}	
	\item Now, we will directly parametrize the policy and will typically
	use $\theta$ to show parameterization:
	$$\pi_\theta (s,a) = \mathbb{P}[ a\mid s; \theta] $$
	\item Goal is to find a policy $\pi$ with the highest value function $V^\pi$
	\item We will focus again on model-free reinforcement learning
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Value-Based vs. Policy-Based RL}

	\begin{itemize}
		\item Value-based
		\begin{itemize}
			\item Learnt value function
			\item Implicit policy (e.g., $\epsilon$-greedy)
		\end{itemize}
		\item Policy-based
		\begin{itemize}
			\item No explicit value function
			\item Learnt policy
		\end{itemize}
		\item Combined: Actor-Critic
		\begin{itemize}
			\item Learnt value function (critic)
			\item Learnt policy (actor)
		\end{itemize}
		
	\end{itemize}		
	
\end{frame}
%-----------------------------------------------------------------------
\begin{frame}[c]{Policy-Based vs Value-Based RL Methods}
Advantages:
\begin{itemize}
    \item Better convergence properties
    \item Effective in high-dimensional or continuous action spaces
    \item Can learn stochastic policies
\end{itemize}

\pause
\bigskip
Disadvantages:
\begin{itemize}
    \item Typically converge to a local rather than a global optimum
    \item Evaluating a policy is typically inefficient and has high variance
\end{itemize}

\end{frame}
%----------------------------------------------------------------------
\begin{frame}[c]{Types of Policies to Search Over}
	
\begin{itemize}
	\item So far have focused on deterministic policies
	\item Now we are thinking about direct policy search in RL
    \item Heavy focus on stochastic policies
\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Example: Rock-Paper-Scissors}
	
	\begin{itemize}
		\item Two-player game of rock-paper-scissors
		\begin{itemize}
			\item Scissors beats paper
			\item Rock beats scissors
			\item Paper beats rock
		\end{itemize}
		\item Let state be the history of prior actions (rock, paper, and scissors) and if
		won or lost
		\item Is deterministic policy optimal? Why or why not?
		\pause
		\item[$\leadsto$] stochastic (random) policy is the Nash equilibrium
	\end{itemize}

	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Example: Aliased Gridword (1)}
	
	\vspace{-1.6em}
	\begin{center}
		\includegraphics[width=0.35\textwidth]{images/gridworld1.png}
	\end{center}
	
	\begin{itemize}
		\item Consider features of the following form (for all N, E, S, W)
		$$\phi(s,a) = 1\text{(s="wall to N", a = "move E")}$$
		\vspace{-1em}
		\begin{itemize}
			\item State representation is not Markov
			\item The agent \alert{cannot} differentiate the gray states
		\end{itemize}
		\item Compare value-based RL, using an approximate value function
		$$Q_\theta (s,a) = f(\phi(s,a); \vect{w})$$
		\item To policy-based RL, using a parameterized policy
		$$\pi_\theta(s,a) = g(\phi(s,a); \theta) $$ 
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Example: Aliased Gridworld (2)}
	
	\begin{center}
		\includegraphics[width=0.5\textwidth]{images/gridworld2.png}
	\end{center}
	
	\begin{itemize}
		\item Under aliasing, an optimal \alert{deterministic} policy will either
		\begin{itemize}
			\item Move W in both gray states 
			\item Move E in both gray states
		\end{itemize}
		\item Either way, it can get stuck and never reach the money
		\item Value-based RL learns a near-deterministic policy (e.g., greedy or $\epsilon$-greedy)
		\item So it will traverse the corridor for a long time
	\end{itemize}

\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Example: Aliased Gridworld (3)}
	
	\begin{center}
		\includegraphics[width=0.5\textwidth]{images/gridworld3.png}
	\end{center}
	
	\begin{itemize}
		\item An optimal \alert{stochastic} policy will randomly move E or W in grey states
		$$\pi_\theta\text{(wall to N and S, move E)} = 0.5 $$
		$$\pi_\theta\text{(wall to N and S, move W)} = 0.5 $$
		\item It will reach the goal state in a few steps with a high probability
		\item Policy-based RL can learn the optimal stochastic policy
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Objective Functions}
	
	\begin{itemize}
		\item Goal: given a policy $\pi_\theta(s,a)$ with parameters $\theta$, find best $\theta^*$
		\begin{itemize}
		    \item \alert{Remark}: Before, we used $\vect{w}$ for the parameters of our approximation. To make it clear that we talk about the parameters of the policy, we use $\theta$ now.
		\end{itemize}
		\item But how do we measure the quality of a given policy $\pi_\theta$?
		\item In continual (i.e., non-episodic) environments, we can use:
		\begin{itemize}
		    \item the average value $\sum_{s} d^{\pi}(s) V^{\pi}(s)$\footnote{$d^\pi(s)$ is the probability being in state $s$ under policy $\pi$}
		    \item or the average reward per time-step $\sum_{s} d^{\pi}(s) \sum_{a} \pi(s,a)r(s,a)$
		\end{itemize}
        \item In episodic environments, we can use the policy value at starting state $\sum_{s_0} \rho(s_0) V(s_0; \theta)$
		\item For simplicity, we will discuss the episodic case.
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%-----------------------------------------------------------------------
\end{document}
