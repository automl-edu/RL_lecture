% !TeX spellcheck = en_US
\documentclass[aspectratio=169]{../latex_main/tntbeamer}  % you can pass all options of the beamer class, e.g., 'handout' or 'aspectratio=43'
\input{../latex_main/preamble}
\title[RL: Score Function and Vanilla GP]{RL: Policy Search}
\subtitle{Score Function and Vanilla Policy Gradient}



\begin{document}
	
	\maketitle

%----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Likelihood Ratio + Score Function Policy Gradient}
From the last slide deck:
	\begin{itemize}
		\item Our goal is to find the policy parameters $\theta^*$
		$$\theta^* \in \argmax_{\theta} V(\theta) = \argmax_{\theta}\sum_{\tau} P(\tau; \theta) R(\tau) $$
		\item Approximate with an empirical estimate for $m$ sample trajectories under
		policy $\pi_\theta$:
		\begin{eqnarray}
		\nabla_\theta V(\theta) &\approx& \frac{1}{m} \sum_{i=1}^{m} R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)}; \theta) \nonumber\\
		&=& \frac{1}{m} \sum_{i=1}^{m} R(\tau^{(i)}) \sum_{t=0}^{T-1} \underbrace{\nabla_\theta \log \pi_\theta (a_t^{(i)} \mid s_t^{(i)})}_{\text{Score Function}}
		\end{eqnarray}
		\item No need to know the dynamics model
		
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Score Function Gradient Estimator: Intuition}
	
	\begin{itemize}
		\item Consider generic form of $R(\tau^{(i)}) \nabla_\theta \log P(\tau^{(i)}; \theta)$:\\
		$\hat{g}(x_i) = f(x_i) \nabla_\theta \log P(x_i \mid \theta)$
		\item $f(x)$ measures how good the sample $x$ is
		\item To increase $\hat{g}(x_i)$ means adapting $\theta$ thereby increasing the probability of $x_i$\\ in proportion of its quality according to f
		\item Valid even if $f(x)$ is discontinuous or unknown;\\ or sample space (containing $x$) is a discrete set
	\end{itemize}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Score Function Gradient Estimator: Intuition}
	

\centering
$\hat{g}_i = f(x_i) \nabla_\theta \log p(x_i \mid \theta)$
\bigskip

\includegraphics[width=0.4\textwidth]{images/scoring_function_1.png}
\pause
\includegraphics[width=0.4\textwidth]{images/scoring_function_2.png}
	
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Policy Gradient Theorem}
	
The policy gradient theorem generalizes the likelihood ratio approach:
\begin{block}{Theorem}
For any differentiable policy $\pi_\theta$, the policy gradient is
$$\nabla_\theta J_\theta= \mathbb{E}_{\pi_\theta} [Q^{\pi_\theta}(s,a) \nabla_\theta \log \pi_\theta(s,a) ] $$
for an objective function $J$.
\end{block}

$\leadsto$ Also known as Vanilla Policy Gradient.
 
\end{frame}
%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recap: RL using Policy Gradients}
	
\begin{itemize}
    \item Initialize a stochastic policy using a differentiable function, such as a DNN.
    \item Sample trajectories using the current policy $\tau_1 , \dots , \tau_m \sim \pi_\theta$.
    \item Estimate the objective $J_\theta$ as the expected return over these trajectories.
    \item Calculate the gradient 
        $$\nabla_\theta J_\theta = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(s,a) Q^{\pi_\theta}(s,a)]$$
    \item Change policy parameters to the ones that maximize the gradient 
        $$\theta^* \in \argmax_\theta \nabla_\theta J(\theta)$$
\end{itemize}
	
\end{frame}

%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recap: Pros of Policy Gradients}
	
\begin{itemize}
    \item We can approximate the policy directly
    \begin{itemize}
        \item In contrast to Q-learning, policy gradient methods do not require the need to estimate the policy by greedily selecting actions with high Q-values
        \item Ideally, we don't need to store the action values in a separate buffer to estimate these values
    \end{itemize}
    \pause
    \item The policies are stochastic
    \begin{itemize}
        \item Since we output a probability distribution over actions, the agent explores the state space without always taking the same trajectory. In Q-learning, we would have to do this using $\epsilon$-greedy exploration.
    \end{itemize}
    \pause
    \item More-effective in high-dimensional and continuous action spaces
    \begin{itemize}
        \item In Q-learning, we assign a Q-value to each possible action and then take the action that has the maximum value, which does not scale well with the number of actions. \item Policy Gradients use probability distributions, which are inherently applicable to continuous actions as well
    \end{itemize}
    \pause
    \item Policy Gradient methods have better convergence properties
\end{itemize}
	
\end{frame}

%-----------------------------------------------------------------------
%----------------------------------------------------------------------
\begin{frame}[c]{Recap: Cons of Policy Gradients}
	
\begin{itemize}
    \item Policy gradient methods frequently converge to a local maximum instead of a global one
    \begin{itemize}
        \item Since they follow the gradient, they suffer from the same issues, such as sensitivity to initialization and the requirement of tuning step size.
        \item Without this hyperparameter tuning, they have more chances of getting stuck at local optimum points.
    \end{itemize}
    \pause
    \item They can take significantly longer to train
    \begin{itemize}
        \item These methods use Monte-Carlo sampling by collecting trajectories and estimating objectives at every step. 
        \item If we compare this to Off-Policy methods such as DQN, off-policy methods can re-use experiences in the replay buffer and, thus, do not need to perform updates at every step.
    \end{itemize}
    \pause
    \item The policy gradient can have a high variance
    \begin{itemize}
        \item Given the stochasticity of the environment, the same starting state can lead to very different returns in the trajectory.
        \item Since policy gradient methods are constantly sampling trajectories, the return estimates can be very different.
    \end{itemize}
\end{itemize}

	
\end{frame}
%-----------------------------------------------------------------------
\end{document}
